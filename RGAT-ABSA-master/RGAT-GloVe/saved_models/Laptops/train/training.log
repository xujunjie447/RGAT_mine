Loading vocab...
Loading vocab from: ../dataset/Biaffine/glove/Laptops/vocab_tok.vocab
Loading vocab from: ../dataset/Biaffine/glove/Laptops/vocab_post.vocab
Loading vocab from: ../dataset/Biaffine/glove/Laptops/vocab_pos.vocab
Loading vocab from: ../dataset/Biaffine/glove/Laptops/vocab_dep.vocab
Loading vocab from: ../dataset/Biaffine/glove/Laptops/vocab_pol.vocab
token_vocab: 3525, post_vocab: 168, pos_vocab: 46, dep_vocab: 34, pol_vocab: 3
Loading pretrained word emb...
Loading 3448/3525 words from vocab...
-----------  Configuration Arguments -----------
alpha: 1.0
att_dropout: 0
attn_heads: 5
batch_size: 32
beta: 1.0
bidirect: True
cross_val_fold: 10
data_dir: ../dataset/Biaffine/glove/Laptops
dep_dim: 30
dep_size: 34
direct: False
emb_dim: 300
glove_dir: /mnt/data2/xfbai/data/embeddings/glove
hidden_dim: 50
input_dropout: 0.7
layer_dropout: 0
log: logs.txt
log_step: 20
loop: True
lower: True
lr: 0.01
model: RGAT
num_class: 3
num_epoch: 60
num_layers: 4
optim: adamax
output_merge: gate
pooling: avg
pos_dim: 30
pos_size: 46
post_dim: 30
post_size: 168
rnn_dropout: 0.1
rnn_hidden: 50
rnn_layers: 1
save_dir: saved_models/Laptops/train
seed: 48
shuffle: True
tok_size: 3525
tune: False
vocab_dir: ../dataset/Biaffine/glove/Laptops
------------------------------------------------
2282 instances loaded from ../dataset/Biaffine/glove/Laptops/train.json
72 batches created for ../dataset/Biaffine/glove/Laptops/train.json
632 instances loaded from ../dataset/Biaffine/glove/Laptops/valid.json
20 batches created for ../dataset/Biaffine/glove/Laptops/valid.json
632 instances loaded from ../dataset/Biaffine/glove/Laptops/test.json
20 batches created for ../dataset/Biaffine/glove/Laptops/test.json
/mnt/data2/xfbai/Anaconda/envs/py36torch1.2new/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
RGATABSA(
  (enc): ABSAEncoder(
    (emb): Embedding(3525, 300, padding_idx=0)
    (pos_emb): Embedding(46, 30, padding_idx=0)
    (post_emb): Embedding(168, 30, padding_idx=0)
    (dep_emb): Embedding(34, 30, padding_idx=0)
    (encoder): DoubleEncoder(
      (emb): Embedding(3525, 300, padding_idx=0)
      (pos_emb): Embedding(46, 30, padding_idx=0)
      (post_emb): Embedding(168, 30, padding_idx=0)
      (dep_emb): Embedding(34, 30, padding_idx=0)
      (Sent_encoder): LSTM(360, 50, batch_first=True, dropout=0.1, bidirectional=True)
      (rnn_drop): Dropout(p=0.1, inplace=False)
      (in_drop): Dropout(p=0.7, inplace=False)
      (graph_encoder): RGATEncoder(
        (transformer): ModuleList(
          (0): RGATLayer(
            (self_attn): MultiHeadedAttention(
              (linear_keys): Linear(in_features=100, out_features=100, bias=True)
              (linear_values): Linear(in_features=100, out_features=100, bias=True)
              (linear_query): Linear(in_features=100, out_features=100, bias=True)
              (linear_structure_k): Linear(in_features=30, out_features=20, bias=True)
              (linear_structure_v): Linear(in_features=30, out_features=20, bias=True)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0, inplace=False)
              (final_linear): Linear(in_features=100, out_features=100, bias=True)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=100, out_features=100, bias=True)
              (w_2): Linear(in_features=100, out_features=100, bias=True)
              (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
              (dropout_1): Dropout(p=0, inplace=False)
              (relu): ReLU()
              (dropout_2): Dropout(p=0, inplace=False)
            )
            (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (1): RGATLayer(
            (self_attn): MultiHeadedAttention(
              (linear_keys): Linear(in_features=100, out_features=100, bias=True)
              (linear_values): Linear(in_features=100, out_features=100, bias=True)
              (linear_query): Linear(in_features=100, out_features=100, bias=True)
              (linear_structure_k): Linear(in_features=30, out_features=20, bias=True)
              (linear_structure_v): Linear(in_features=30, out_features=20, bias=True)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0, inplace=False)
              (final_linear): Linear(in_features=100, out_features=100, bias=True)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=100, out_features=100, bias=True)
              (w_2): Linear(in_features=100, out_features=100, bias=True)
              (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
              (dropout_1): Dropout(p=0, inplace=False)
              (relu): ReLU()
              (dropout_2): Dropout(p=0, inplace=False)
            )
            (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (2): RGATLayer(
            (self_attn): MultiHeadedAttention(
              (linear_keys): Linear(in_features=100, out_features=100, bias=True)
              (linear_values): Linear(in_features=100, out_features=100, bias=True)
              (linear_query): Linear(in_features=100, out_features=100, bias=True)
              (linear_structure_k): Linear(in_features=30, out_features=20, bias=True)
              (linear_structure_v): Linear(in_features=30, out_features=20, bias=True)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0, inplace=False)
              (final_linear): Linear(in_features=100, out_features=100, bias=True)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=100, out_features=100, bias=True)
              (w_2): Linear(in_features=100, out_features=100, bias=True)
              (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
              (dropout_1): Dropout(p=0, inplace=False)
              (relu): ReLU()
              (dropout_2): Dropout(p=0, inplace=False)
            )
            (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (3): RGATLayer(
            (self_attn): MultiHeadedAttention(
              (linear_keys): Linear(in_features=100, out_features=100, bias=True)
              (linear_values): Linear(in_features=100, out_features=100, bias=True)
              (linear_query): Linear(in_features=100, out_features=100, bias=True)
              (linear_structure_k): Linear(in_features=30, out_features=20, bias=True)
              (linear_structure_v): Linear(in_features=30, out_features=20, bias=True)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0, inplace=False)
              (final_linear): Linear(in_features=100, out_features=100, bias=True)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=100, out_features=100, bias=True)
              (w_2): Linear(in_features=100, out_features=100, bias=True)
              (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
              (dropout_1): Dropout(p=0, inplace=False)
              (relu): ReLU()
              (dropout_2): Dropout(p=0, inplace=False)
            )
            (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
        (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
      )
      (out_map): Linear(in_features=100, out_features=50, bias=True)
    )
    (inp_map): Linear(in_features=100, out_features=50, bias=True)
    (out_gate_map): Linear(in_features=100, out_features=50, bias=True)
  )
  (classifier): Linear(in_features=50, out_features=3, bias=True)
)
Total parameters: 1494203
Training Set: 72
Valid Set: 20
Test Set: 20
Epoch 1------------------------------------------------------------
19/72 train_loss: 1.084012, train_acc: 42.187500
39/72 train_loss: 1.053539, train_acc: 45.781250
59/72 train_loss: 1.032678, train_acc: 47.656250
End of 1 train_loss: 1.0180, train_acc: 49.3142, val_loss: 0.8381, val_acc: 55.2604, f1_score: 0.4946
new best model saved.
Epoch 2------------------------------------------------------------
19/72 train_loss: 0.893035, train_acc: 60.156250
39/72 train_loss: 0.896852, train_acc: 58.828125
59/72 train_loss: 0.880996, train_acc: 59.791667
End of 2 train_loss: 0.8715, train_acc: 60.0955, val_loss: 0.7470, val_acc: 66.0938, f1_score: 0.5919
new best model saved.
Epoch 3------------------------------------------------------------
19/72 train_loss: 0.812635, train_acc: 65.156250
39/72 train_loss: 0.822711, train_acc: 63.359375
59/72 train_loss: 0.816634, train_acc: 63.958333
End of 3 train_loss: 0.8095, train_acc: 64.2622, val_loss: 0.7418, val_acc: 66.0417, f1_score: 0.6056
Epoch 4------------------------------------------------------------
19/72 train_loss: 0.776252, train_acc: 67.187500
39/72 train_loss: 0.772294, train_acc: 65.703125
59/72 train_loss: 0.766295, train_acc: 66.718750
End of 4 train_loss: 0.7564, train_acc: 67.3524, val_loss: 0.7986, val_acc: 63.5417, f1_score: 0.5796
Epoch 5------------------------------------------------------------
19/72 train_loss: 0.753368, train_acc: 66.875000
39/72 train_loss: 0.751416, train_acc: 67.109375
59/72 train_loss: 0.738799, train_acc: 66.927083
End of 5 train_loss: 0.7294, train_acc: 68.0990, val_loss: 0.7409, val_acc: 66.2500, f1_score: 0.6087
new best model saved.
Epoch 6------------------------------------------------------------
19/72 train_loss: 0.704339, train_acc: 69.218750
39/72 train_loss: 0.705914, train_acc: 68.437500
59/72 train_loss: 0.700339, train_acc: 69.114583
End of 6 train_loss: 0.6984, train_acc: 69.3403, val_loss: 0.6964, val_acc: 70.0000, f1_score: 0.6488
new best model saved.
Epoch 7------------------------------------------------------------
19/72 train_loss: 0.641624, train_acc: 72.968750
39/72 train_loss: 0.673043, train_acc: 71.250000
59/72 train_loss: 0.661505, train_acc: 71.562500
End of 7 train_loss: 0.6561, train_acc: 72.3524, val_loss: 0.6948, val_acc: 72.2396, f1_score: 0.6722
new best model saved.
Epoch 8------------------------------------------------------------
19/72 train_loss: 0.630710, train_acc: 74.843750
39/72 train_loss: 0.660649, train_acc: 71.328125
59/72 train_loss: 0.654948, train_acc: 72.031250
End of 8 train_loss: 0.6560, train_acc: 72.1701, val_loss: 0.6892, val_acc: 71.7708, f1_score: 0.6645
Epoch 9------------------------------------------------------------
19/72 train_loss: 0.619444, train_acc: 73.593750
39/72 train_loss: 0.613855, train_acc: 74.296875
59/72 train_loss: 0.606724, train_acc: 74.583333
End of 9 train_loss: 0.6113, train_acc: 74.7309, val_loss: 0.6897, val_acc: 72.2396, f1_score: 0.6743
Epoch 10------------------------------------------------------------
19/72 train_loss: 0.579150, train_acc: 76.406250
39/72 train_loss: 0.598229, train_acc: 75.156250
59/72 train_loss: 0.588866, train_acc: 75.677083
End of 10 train_loss: 0.5895, train_acc: 75.9983, val_loss: 0.7092, val_acc: 72.4479, f1_score: 0.6765
new best model saved.
Epoch 11------------------------------------------------------------
19/72 train_loss: 0.636434, train_acc: 73.593750
39/72 train_loss: 0.619407, train_acc: 73.906250
59/72 train_loss: 0.595654, train_acc: 75.416667
End of 11 train_loss: 0.5911, train_acc: 75.2431, val_loss: 0.7413, val_acc: 72.1875, f1_score: 0.6742
Epoch 12------------------------------------------------------------
19/72 train_loss: 0.589320, train_acc: 76.875000
39/72 train_loss: 0.594113, train_acc: 75.468750
59/72 train_loss: 0.582689, train_acc: 75.885417
End of 12 train_loss: 0.5765, train_acc: 76.4236, val_loss: 0.7365, val_acc: 70.7812, f1_score: 0.6637
Epoch 13------------------------------------------------------------
19/72 train_loss: 0.586685, train_acc: 75.937500
39/72 train_loss: 0.575929, train_acc: 76.484375
59/72 train_loss: 0.563037, train_acc: 76.510417
End of 13 train_loss: 0.5625, train_acc: 76.7708, val_loss: 0.7514, val_acc: 72.2396, f1_score: 0.6760
Epoch 14------------------------------------------------------------
19/72 train_loss: 0.570443, train_acc: 75.625000
39/72 train_loss: 0.575884, train_acc: 75.156250
59/72 train_loss: 0.554835, train_acc: 76.302083
End of 14 train_loss: 0.5545, train_acc: 76.5885, val_loss: 0.7778, val_acc: 72.6562, f1_score: 0.6764
new best model saved.
Epoch 15------------------------------------------------------------
19/72 train_loss: 0.548753, train_acc: 78.281250
39/72 train_loss: 0.550922, train_acc: 78.125000
59/72 train_loss: 0.531120, train_acc: 78.593750
End of 15 train_loss: 0.5256, train_acc: 78.8108, val_loss: 0.8482, val_acc: 71.0417, f1_score: 0.6694
Epoch 16------------------------------------------------------------
19/72 train_loss: 0.495636, train_acc: 79.062500
39/72 train_loss: 0.491558, train_acc: 79.609375
59/72 train_loss: 0.503209, train_acc: 78.854167
End of 16 train_loss: 0.4956, train_acc: 79.3837, val_loss: 0.7414, val_acc: 74.8958, f1_score: 0.7066
new best model saved.
Epoch 17------------------------------------------------------------
19/72 train_loss: 0.519177, train_acc: 78.750000
39/72 train_loss: 0.509485, train_acc: 78.984375
59/72 train_loss: 0.494630, train_acc: 79.583333
End of 17 train_loss: 0.4897, train_acc: 79.8611, val_loss: 0.8637, val_acc: 70.9896, f1_score: 0.6630
Epoch 18------------------------------------------------------------
19/72 train_loss: 0.480213, train_acc: 79.843750
39/72 train_loss: 0.480722, train_acc: 79.531250
59/72 train_loss: 0.494426, train_acc: 79.270833
End of 18 train_loss: 0.4861, train_acc: 79.8177, val_loss: 0.7524, val_acc: 73.4896, f1_score: 0.6954
Epoch 19------------------------------------------------------------
19/72 train_loss: 0.476162, train_acc: 80.625000
39/72 train_loss: 0.477772, train_acc: 80.546875
59/72 train_loss: 0.477022, train_acc: 80.677083
End of 19 train_loss: 0.4776, train_acc: 80.5990, val_loss: 0.7764, val_acc: 71.7708, f1_score: 0.6785
Epoch 20------------------------------------------------------------
19/72 train_loss: 0.466685, train_acc: 82.187500
39/72 train_loss: 0.463915, train_acc: 81.640625
59/72 train_loss: 0.463598, train_acc: 81.875000
End of 20 train_loss: 0.4634, train_acc: 81.8490, val_loss: 0.7387, val_acc: 72.5000, f1_score: 0.6833
Epoch 21------------------------------------------------------------
19/72 train_loss: 0.489626, train_acc: 79.375000
39/72 train_loss: 0.467311, train_acc: 80.234375
59/72 train_loss: 0.457448, train_acc: 81.041667
End of 21 train_loss: 0.4470, train_acc: 81.6233, val_loss: 0.9736, val_acc: 73.1771, f1_score: 0.6858
Epoch 22------------------------------------------------------------
19/72 train_loss: 0.446030, train_acc: 82.343750
39/72 train_loss: 0.462883, train_acc: 81.250000
59/72 train_loss: 0.453535, train_acc: 81.718750
End of 22 train_loss: 0.4439, train_acc: 81.8056, val_loss: 0.9412, val_acc: 71.4062, f1_score: 0.6650
Epoch 23------------------------------------------------------------
19/72 train_loss: 0.450396, train_acc: 83.750000
39/72 train_loss: 0.435093, train_acc: 82.656250
59/72 train_loss: 0.421204, train_acc: 83.072917
End of 23 train_loss: 0.4147, train_acc: 83.6372, val_loss: 0.8249, val_acc: 74.2708, f1_score: 0.6985
Epoch 24------------------------------------------------------------
19/72 train_loss: 0.441255, train_acc: 82.500000
39/72 train_loss: 0.446113, train_acc: 82.578125
59/72 train_loss: 0.435719, train_acc: 82.604167
End of 24 train_loss: 0.4311, train_acc: 82.6389, val_loss: 0.9121, val_acc: 70.5208, f1_score: 0.6620
Epoch 25------------------------------------------------------------
19/72 train_loss: 0.435149, train_acc: 82.343750
39/72 train_loss: 0.427575, train_acc: 82.187500
59/72 train_loss: 0.426475, train_acc: 82.447917
End of 25 train_loss: 0.4185, train_acc: 82.7691, val_loss: 0.8092, val_acc: 73.9583, f1_score: 0.6908
Epoch 26------------------------------------------------------------
19/72 train_loss: 0.393308, train_acc: 84.375000
39/72 train_loss: 0.404982, train_acc: 83.671875
59/72 train_loss: 0.419504, train_acc: 82.968750
End of 26 train_loss: 0.4156, train_acc: 83.0642, val_loss: 0.8268, val_acc: 71.9271, f1_score: 0.6684
Epoch 27------------------------------------------------------------
19/72 train_loss: 0.413162, train_acc: 83.750000
39/72 train_loss: 0.421225, train_acc: 82.734375
59/72 train_loss: 0.410925, train_acc: 83.020833
End of 27 train_loss: 0.4073, train_acc: 83.7240, val_loss: 0.8494, val_acc: 73.6458, f1_score: 0.6882
Epoch 28------------------------------------------------------------
19/72 train_loss: 0.377017, train_acc: 85.156250
39/72 train_loss: 0.402361, train_acc: 83.828125
59/72 train_loss: 0.404919, train_acc: 83.750000
End of 28 train_loss: 0.4020, train_acc: 83.9410, val_loss: 0.8612, val_acc: 74.2708, f1_score: 0.6964
Epoch 29------------------------------------------------------------
19/72 train_loss: 0.382716, train_acc: 84.687500
39/72 train_loss: 0.409681, train_acc: 83.359375
59/72 train_loss: 0.412171, train_acc: 83.281250
End of 29 train_loss: 0.4067, train_acc: 83.7240, val_loss: 0.8815, val_acc: 72.3958, f1_score: 0.6829
Epoch 30------------------------------------------------------------
19/72 train_loss: 0.373947, train_acc: 85.000000
39/72 train_loss: 0.380250, train_acc: 85.000000
59/72 train_loss: 0.373278, train_acc: 85.208333
End of 30 train_loss: 0.3731, train_acc: 85.1910, val_loss: 0.8890, val_acc: 73.4375, f1_score: 0.6904
Epoch 31------------------------------------------------------------
19/72 train_loss: 0.337027, train_acc: 85.781250
39/72 train_loss: 0.359885, train_acc: 84.531250
59/72 train_loss: 0.361267, train_acc: 84.635417
End of 31 train_loss: 0.3531, train_acc: 84.8872, val_loss: 0.8759, val_acc: 73.8542, f1_score: 0.6970
Epoch 32------------------------------------------------------------
19/72 train_loss: 0.350380, train_acc: 85.468750
39/72 train_loss: 0.390929, train_acc: 84.296875
59/72 train_loss: 0.380815, train_acc: 84.427083
End of 32 train_loss: 0.3854, train_acc: 84.5399, val_loss: 0.8378, val_acc: 73.8542, f1_score: 0.6923
Epoch 33------------------------------------------------------------
19/72 train_loss: 0.343181, train_acc: 87.187500
39/72 train_loss: 0.369152, train_acc: 86.093750
59/72 train_loss: 0.379630, train_acc: 85.416667
End of 33 train_loss: 0.3724, train_acc: 85.5382, val_loss: 0.8255, val_acc: 73.4896, f1_score: 0.6915
Epoch 34------------------------------------------------------------
19/72 train_loss: 0.326611, train_acc: 85.312500
39/72 train_loss: 0.334325, train_acc: 86.640625
59/72 train_loss: 0.337330, train_acc: 86.250000
End of 34 train_loss: 0.3457, train_acc: 85.8420, val_loss: 0.8358, val_acc: 73.4375, f1_score: 0.6910
Epoch 35------------------------------------------------------------
19/72 train_loss: 0.325593, train_acc: 87.031250
39/72 train_loss: 0.330186, train_acc: 86.484375
59/72 train_loss: 0.332563, train_acc: 86.510417
End of 35 train_loss: 0.3296, train_acc: 86.7188, val_loss: 0.8808, val_acc: 75.0000, f1_score: 0.7128
new best model saved.
Epoch 36------------------------------------------------------------
19/72 train_loss: 0.324284, train_acc: 87.187500
39/72 train_loss: 0.340693, train_acc: 86.640625
59/72 train_loss: 0.342089, train_acc: 86.770833
End of 36 train_loss: 0.3444, train_acc: 86.5799, val_loss: 0.8284, val_acc: 74.2708, f1_score: 0.7035
Epoch 37------------------------------------------------------------
19/72 train_loss: 0.371096, train_acc: 85.156250
39/72 train_loss: 0.332782, train_acc: 86.718750
59/72 train_loss: 0.330080, train_acc: 86.927083
End of 37 train_loss: 0.3261, train_acc: 86.7014, val_loss: 0.8678, val_acc: 74.3750, f1_score: 0.6980
Epoch 38------------------------------------------------------------
19/72 train_loss: 0.326956, train_acc: 87.343750
39/72 train_loss: 0.343353, train_acc: 86.796875
59/72 train_loss: 0.340560, train_acc: 86.927083
End of 38 train_loss: 0.3279, train_acc: 87.5868, val_loss: 0.8325, val_acc: 75.2083, f1_score: 0.7127
new best model saved.
Epoch 39------------------------------------------------------------
19/72 train_loss: 0.349138, train_acc: 85.312500
39/72 train_loss: 0.361242, train_acc: 85.234375
59/72 train_loss: 0.365637, train_acc: 84.947917
End of 39 train_loss: 0.3632, train_acc: 85.4514, val_loss: 0.8113, val_acc: 73.6979, f1_score: 0.6931
Epoch 40------------------------------------------------------------
19/72 train_loss: 0.306427, train_acc: 88.125000
39/72 train_loss: 0.295919, train_acc: 88.671875
59/72 train_loss: 0.313661, train_acc: 87.604167
End of 40 train_loss: 0.3149, train_acc: 87.4045, val_loss: 0.8783, val_acc: 74.5833, f1_score: 0.7002
Epoch 41------------------------------------------------------------
19/72 train_loss: 0.328678, train_acc: 87.812500
39/72 train_loss: 0.319790, train_acc: 87.734375
59/72 train_loss: 0.323002, train_acc: 87.291667
End of 41 train_loss: 0.3203, train_acc: 87.3264, val_loss: 0.8315, val_acc: 73.9062, f1_score: 0.6948
Epoch 42------------------------------------------------------------
19/72 train_loss: 0.310363, train_acc: 88.750000
39/72 train_loss: 0.322571, train_acc: 87.734375
59/72 train_loss: 0.311008, train_acc: 87.708333
End of 42 train_loss: 0.3105, train_acc: 87.6128, val_loss: 0.9178, val_acc: 72.8125, f1_score: 0.6829
Epoch 43------------------------------------------------------------
19/72 train_loss: 0.321041, train_acc: 86.562500
39/72 train_loss: 0.329918, train_acc: 86.171875
59/72 train_loss: 0.321705, train_acc: 86.302083
End of 43 train_loss: 0.3235, train_acc: 86.3889, val_loss: 0.8773, val_acc: 74.2708, f1_score: 0.6981
Epoch 44------------------------------------------------------------
19/72 train_loss: 0.307381, train_acc: 87.812500
39/72 train_loss: 0.292497, train_acc: 88.750000
59/72 train_loss: 0.292331, train_acc: 88.802083
End of 44 train_loss: 0.2894, train_acc: 88.6285, val_loss: 0.9187, val_acc: 74.6875, f1_score: 0.6976
Epoch 45------------------------------------------------------------
19/72 train_loss: 0.311955, train_acc: 86.562500
39/72 train_loss: 0.328634, train_acc: 86.015625
59/72 train_loss: 0.332197, train_acc: 86.354167
End of 45 train_loss: 0.3248, train_acc: 86.7188, val_loss: 0.9069, val_acc: 74.2188, f1_score: 0.6959
Epoch 46------------------------------------------------------------
19/72 train_loss: 0.298295, train_acc: 87.812500
39/72 train_loss: 0.308093, train_acc: 87.343750
59/72 train_loss: 0.305650, train_acc: 87.708333
End of 46 train_loss: 0.3010, train_acc: 87.8906, val_loss: 0.9010, val_acc: 74.1667, f1_score: 0.6909
Epoch 47------------------------------------------------------------
19/72 train_loss: 0.289209, train_acc: 87.187500
39/72 train_loss: 0.294830, train_acc: 87.421875
59/72 train_loss: 0.299531, train_acc: 87.500000
End of 47 train_loss: 0.2986, train_acc: 87.7604, val_loss: 0.8792, val_acc: 74.1667, f1_score: 0.6932
Epoch 48------------------------------------------------------------
19/72 train_loss: 0.258922, train_acc: 89.531250
39/72 train_loss: 0.287792, train_acc: 88.515625
59/72 train_loss: 0.288829, train_acc: 88.645833
End of 48 train_loss: 0.2778, train_acc: 89.2795, val_loss: 0.9354, val_acc: 74.8438, f1_score: 0.6942
Epoch 49------------------------------------------------------------
19/72 train_loss: 0.278802, train_acc: 88.281250
39/72 train_loss: 0.299751, train_acc: 87.812500
59/72 train_loss: 0.286254, train_acc: 88.541667
End of 49 train_loss: 0.2844, train_acc: 88.7587, val_loss: 0.8947, val_acc: 75.4167, f1_score: 0.7130
new best model saved.
Epoch 50------------------------------------------------------------
19/72 train_loss: 0.308555, train_acc: 88.281250
39/72 train_loss: 0.296968, train_acc: 88.359375
59/72 train_loss: 0.292433, train_acc: 88.020833
End of 50 train_loss: 0.2895, train_acc: 88.3247, val_loss: 0.9136, val_acc: 75.3646, f1_score: 0.7075
Epoch 51------------------------------------------------------------
19/72 train_loss: 0.281540, train_acc: 88.281250
39/72 train_loss: 0.281331, train_acc: 89.296875
59/72 train_loss: 0.288688, train_acc: 89.010417
End of 51 train_loss: 0.2979, train_acc: 88.6285, val_loss: 0.8305, val_acc: 75.8333, f1_score: 0.7103
new best model saved.
Epoch 52------------------------------------------------------------
19/72 train_loss: 0.227465, train_acc: 91.406250
39/72 train_loss: 0.242298, train_acc: 90.859375
59/72 train_loss: 0.242138, train_acc: 90.625000
End of 52 train_loss: 0.2495, train_acc: 90.2691, val_loss: 0.9126, val_acc: 78.0208, f1_score: 0.7400
new best model saved.
Epoch 53------------------------------------------------------------
19/72 train_loss: 0.283199, train_acc: 90.312500
39/72 train_loss: 0.280076, train_acc: 89.609375
59/72 train_loss: 0.284064, train_acc: 89.114583
End of 53 train_loss: 0.2763, train_acc: 89.4097, val_loss: 0.8926, val_acc: 76.9271, f1_score: 0.7265
Epoch 54------------------------------------------------------------
19/72 train_loss: 0.300277, train_acc: 88.593750
39/72 train_loss: 0.296950, train_acc: 88.828125
59/72 train_loss: 0.287784, train_acc: 89.114583
End of 54 train_loss: 0.2848, train_acc: 89.0972, val_loss: 0.9540, val_acc: 76.4062, f1_score: 0.7145
Epoch 55------------------------------------------------------------
19/72 train_loss: 0.248246, train_acc: 90.156250
39/72 train_loss: 0.266026, train_acc: 89.062500
59/72 train_loss: 0.269108, train_acc: 89.010417
End of 55 train_loss: 0.2682, train_acc: 89.0191, val_loss: 0.9291, val_acc: 76.1458, f1_score: 0.7168
Epoch 56------------------------------------------------------------
19/72 train_loss: 0.321098, train_acc: 87.812500
39/72 train_loss: 0.296704, train_acc: 87.734375
59/72 train_loss: 0.270874, train_acc: 89.270833
End of 56 train_loss: 0.2711, train_acc: 89.2795, val_loss: 0.9639, val_acc: 76.1458, f1_score: 0.7176
Epoch 57------------------------------------------------------------
19/72 train_loss: 0.237170, train_acc: 91.093750
39/72 train_loss: 0.250544, train_acc: 90.703125
59/72 train_loss: 0.249898, train_acc: 90.416667
End of 57 train_loss: 0.2428, train_acc: 90.8854, val_loss: 0.9945, val_acc: 75.3125, f1_score: 0.7047
Epoch 58------------------------------------------------------------
19/72 train_loss: 0.252343, train_acc: 90.468750
39/72 train_loss: 0.270073, train_acc: 89.140625
59/72 train_loss: 0.255558, train_acc: 90.156250
End of 58 train_loss: 0.2489, train_acc: 90.4080, val_loss: 1.0368, val_acc: 74.6875, f1_score: 0.6977
Epoch 59------------------------------------------------------------
19/72 train_loss: 0.241832, train_acc: 91.093750
39/72 train_loss: 0.245604, train_acc: 91.250000
59/72 train_loss: 0.235533, train_acc: 91.302083
End of 59 train_loss: 0.2394, train_acc: 91.0156, val_loss: 1.0066, val_acc: 75.3646, f1_score: 0.7051
Epoch 60------------------------------------------------------------
19/72 train_loss: 0.201620, train_acc: 91.093750
39/72 train_loss: 0.251271, train_acc: 89.140625
59/72 train_loss: 0.250236, train_acc: 89.791667
End of 60 train_loss: 0.2412, train_acc: 90.2344, val_loss: 0.9430, val_acc: 75.7292, f1_score: 0.7099
Training ended with 60 epochs.
Loading best checkpoints from saved_models/Laptops/train/best_checkpoint.pt
Evaluation Results: test_loss:0.9125832915306091, test_acc:78.02083333333334, test_f1:0.7399718710997885
