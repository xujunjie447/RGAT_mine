Loading vocab...
Loading vocab from: ../dataset/Biaffine/glove/Tweets/vocab_tok.vocab
Loading vocab from: ../dataset/Biaffine/glove/Tweets/vocab_post.vocab
Loading vocab from: ../dataset/Biaffine/glove/Tweets/vocab_pos.vocab
Loading vocab from: ../dataset/Biaffine/glove/Tweets/vocab_dep.vocab
Loading vocab from: ../dataset/Biaffine/glove/Tweets/vocab_pol.vocab
token_vocab: 13146, post_vocab: 94, pos_vocab: 47, dep_vocab: 35, pol_vocab: 3
Loading pretrained word emb...
Loading 11320/13146 words from vocab...
-----------  Configuration Arguments -----------
alpha: 1.0
att_dropout: 0
attn_heads: 5
batch_size: 32
beta: 1.0
bidirect: True
cross_val_fold: 10
data_dir: ../dataset/Biaffine/glove/Tweets
dep_dim: 30
dep_size: 35
direct: False
emb_dim: 300
glove_dir: /mnt/data2/xfbai/data/embeddings/glove
hidden_dim: 50
input_dropout: 0.7
layer_dropout: 0
log: logs.txt
log_step: 20
loop: True
lower: True
lr: 0.01
model: RGAT
num_class: 3
num_epoch: 60
num_layers: 6
optim: adamax
output_merge: gate
pooling: avg
pos_dim: 30
pos_size: 47
post_dim: 30
post_size: 94
rnn_dropout: 0.1
rnn_hidden: 50
rnn_layers: 1
save_dir: saved_models/Tweets/train
seed: 22
shuffle: True
tok_size: 13146
tune: False
vocab_dir: ../dataset/Biaffine/glove/Tweets
------------------------------------------------
6051 instances loaded from ../dataset/Biaffine/glove/Tweets/train.json
190 batches created for ../dataset/Biaffine/glove/Tweets/train.json
677 instances loaded from ../dataset/Biaffine/glove/Tweets/valid.json
22 batches created for ../dataset/Biaffine/glove/Tweets/valid.json
677 instances loaded from ../dataset/Biaffine/glove/Tweets/test.json
22 batches created for ../dataset/Biaffine/glove/Tweets/test.json
/mnt/data2/xfbai/Anaconda/envs/py36torch1.2new/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
RGATABSA(
  (enc): ABSAEncoder(
    (emb): Embedding(13146, 300, padding_idx=0)
    (pos_emb): Embedding(47, 30, padding_idx=0)
    (post_emb): Embedding(94, 30, padding_idx=0)
    (dep_emb): Embedding(35, 30, padding_idx=0)
    (encoder): DoubleEncoder(
      (emb): Embedding(13146, 300, padding_idx=0)
      (pos_emb): Embedding(47, 30, padding_idx=0)
      (post_emb): Embedding(94, 30, padding_idx=0)
      (dep_emb): Embedding(35, 30, padding_idx=0)
      (Sent_encoder): LSTM(360, 50, batch_first=True, dropout=0.1, bidirectional=True)
      (rnn_drop): Dropout(p=0.1, inplace=False)
      (in_drop): Dropout(p=0.7, inplace=False)
      (graph_encoder): RGATEncoder(
        (transformer): ModuleList(
          (0): RGATLayer(
            (self_attn): MultiHeadedAttention(
              (linear_keys): Linear(in_features=100, out_features=100, bias=True)
              (linear_values): Linear(in_features=100, out_features=100, bias=True)
              (linear_query): Linear(in_features=100, out_features=100, bias=True)
              (linear_structure_k): Linear(in_features=30, out_features=20, bias=True)
              (linear_structure_v): Linear(in_features=30, out_features=20, bias=True)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0, inplace=False)
              (final_linear): Linear(in_features=100, out_features=100, bias=True)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=100, out_features=100, bias=True)
              (w_2): Linear(in_features=100, out_features=100, bias=True)
              (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
              (dropout_1): Dropout(p=0, inplace=False)
              (relu): ReLU()
              (dropout_2): Dropout(p=0, inplace=False)
            )
            (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (1): RGATLayer(
            (self_attn): MultiHeadedAttention(
              (linear_keys): Linear(in_features=100, out_features=100, bias=True)
              (linear_values): Linear(in_features=100, out_features=100, bias=True)
              (linear_query): Linear(in_features=100, out_features=100, bias=True)
              (linear_structure_k): Linear(in_features=30, out_features=20, bias=True)
              (linear_structure_v): Linear(in_features=30, out_features=20, bias=True)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0, inplace=False)
              (final_linear): Linear(in_features=100, out_features=100, bias=True)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=100, out_features=100, bias=True)
              (w_2): Linear(in_features=100, out_features=100, bias=True)
              (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
              (dropout_1): Dropout(p=0, inplace=False)
              (relu): ReLU()
              (dropout_2): Dropout(p=0, inplace=False)
            )
            (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (2): RGATLayer(
            (self_attn): MultiHeadedAttention(
              (linear_keys): Linear(in_features=100, out_features=100, bias=True)
              (linear_values): Linear(in_features=100, out_features=100, bias=True)
              (linear_query): Linear(in_features=100, out_features=100, bias=True)
              (linear_structure_k): Linear(in_features=30, out_features=20, bias=True)
              (linear_structure_v): Linear(in_features=30, out_features=20, bias=True)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0, inplace=False)
              (final_linear): Linear(in_features=100, out_features=100, bias=True)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=100, out_features=100, bias=True)
              (w_2): Linear(in_features=100, out_features=100, bias=True)
              (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
              (dropout_1): Dropout(p=0, inplace=False)
              (relu): ReLU()
              (dropout_2): Dropout(p=0, inplace=False)
            )
            (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (3): RGATLayer(
            (self_attn): MultiHeadedAttention(
              (linear_keys): Linear(in_features=100, out_features=100, bias=True)
              (linear_values): Linear(in_features=100, out_features=100, bias=True)
              (linear_query): Linear(in_features=100, out_features=100, bias=True)
              (linear_structure_k): Linear(in_features=30, out_features=20, bias=True)
              (linear_structure_v): Linear(in_features=30, out_features=20, bias=True)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0, inplace=False)
              (final_linear): Linear(in_features=100, out_features=100, bias=True)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=100, out_features=100, bias=True)
              (w_2): Linear(in_features=100, out_features=100, bias=True)
              (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
              (dropout_1): Dropout(p=0, inplace=False)
              (relu): ReLU()
              (dropout_2): Dropout(p=0, inplace=False)
            )
            (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (4): RGATLayer(
            (self_attn): MultiHeadedAttention(
              (linear_keys): Linear(in_features=100, out_features=100, bias=True)
              (linear_values): Linear(in_features=100, out_features=100, bias=True)
              (linear_query): Linear(in_features=100, out_features=100, bias=True)
              (linear_structure_k): Linear(in_features=30, out_features=20, bias=True)
              (linear_structure_v): Linear(in_features=30, out_features=20, bias=True)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0, inplace=False)
              (final_linear): Linear(in_features=100, out_features=100, bias=True)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=100, out_features=100, bias=True)
              (w_2): Linear(in_features=100, out_features=100, bias=True)
              (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
              (dropout_1): Dropout(p=0, inplace=False)
              (relu): ReLU()
              (dropout_2): Dropout(p=0, inplace=False)
            )
            (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (5): RGATLayer(
            (self_attn): MultiHeadedAttention(
              (linear_keys): Linear(in_features=100, out_features=100, bias=True)
              (linear_values): Linear(in_features=100, out_features=100, bias=True)
              (linear_query): Linear(in_features=100, out_features=100, bias=True)
              (linear_structure_k): Linear(in_features=30, out_features=20, bias=True)
              (linear_structure_v): Linear(in_features=30, out_features=20, bias=True)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0, inplace=False)
              (final_linear): Linear(in_features=100, out_features=100, bias=True)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=100, out_features=100, bias=True)
              (w_2): Linear(in_features=100, out_features=100, bias=True)
              (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
              (dropout_1): Dropout(p=0, inplace=False)
              (relu): ReLU()
              (dropout_2): Dropout(p=0, inplace=False)
            )
            (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
        (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
      )
      (out_map): Linear(in_features=100, out_features=50, bias=True)
    )
    (inp_map): Linear(in_features=100, out_features=50, bias=True)
    (out_gate_map): Linear(in_features=100, out_features=50, bias=True)
  )
  (classifier): Linear(in_features=50, out_features=3, bias=True)
)
Total parameters: 4502823
Training Set: 190
Valid Set: 22
Test Set: 22
Epoch 1------------------------------------------------------------
19/190 train_loss: 1.127784, train_acc: 40.156250
39/190 train_loss: 1.071004, train_acc: 46.406250
59/190 train_loss: 1.041116, train_acc: 49.010417
79/190 train_loss: 1.027429, train_acc: 50.195312
99/190 train_loss: 1.018218, train_acc: 50.093750
119/190 train_loss: 1.008441, train_acc: 50.520833
139/190 train_loss: 0.994232, train_acc: 51.272321
159/190 train_loss: 0.990183, train_acc: 51.542969
179/190 train_loss: 0.986065, train_acc: 51.961806
End of 1 train_loss: 0.9819, train_acc: 52.3849, val_loss: 0.8789, val_acc: 60.3125, f1_score: 0.5541
new best model saved.
Epoch 2------------------------------------------------------------
19/190 train_loss: 0.955114, train_acc: 54.531250
39/190 train_loss: 0.932262, train_acc: 57.109375
59/190 train_loss: 0.917930, train_acc: 58.020833
79/190 train_loss: 0.910731, train_acc: 58.828125
99/190 train_loss: 0.907558, train_acc: 58.625000
119/190 train_loss: 0.908508, train_acc: 58.437500
139/190 train_loss: 0.900140, train_acc: 59.263393
159/190 train_loss: 0.895831, train_acc: 59.531250
179/190 train_loss: 0.890256, train_acc: 59.548611
End of 2 train_loss: 0.8888, train_acc: 59.6107, val_loss: 0.8155, val_acc: 63.7216, f1_score: 0.6057
new best model saved.
Epoch 3------------------------------------------------------------
19/190 train_loss: 0.875534, train_acc: 58.750000
39/190 train_loss: 0.867521, train_acc: 58.671875
59/190 train_loss: 0.863918, train_acc: 59.218750
79/190 train_loss: 0.845528, train_acc: 60.625000
99/190 train_loss: 0.843131, train_acc: 60.750000
119/190 train_loss: 0.844765, train_acc: 60.390625
139/190 train_loss: 0.837842, train_acc: 61.093750
159/190 train_loss: 0.832250, train_acc: 61.621094
179/190 train_loss: 0.829868, train_acc: 61.944444
End of 3 train_loss: 0.8304, train_acc: 61.9134, val_loss: 0.7685, val_acc: 67.2727, f1_score: 0.6307
new best model saved.
Epoch 4------------------------------------------------------------
19/190 train_loss: 0.846407, train_acc: 60.156250
39/190 train_loss: 0.824877, train_acc: 62.187500
59/190 train_loss: 0.823842, train_acc: 62.083333
79/190 train_loss: 0.808284, train_acc: 62.890625
99/190 train_loss: 0.797719, train_acc: 63.968750
119/190 train_loss: 0.796653, train_acc: 63.906250
139/190 train_loss: 0.791103, train_acc: 64.620536
159/190 train_loss: 0.789390, train_acc: 64.765625
179/190 train_loss: 0.787189, train_acc: 64.947917
End of 4 train_loss: 0.7838, train_acc: 65.1974, val_loss: 0.7574, val_acc: 67.4148, f1_score: 0.6553
new best model saved.
Epoch 5------------------------------------------------------------
19/190 train_loss: 0.798299, train_acc: 63.593750
39/190 train_loss: 0.795318, train_acc: 64.453125
59/190 train_loss: 0.790473, train_acc: 65.468750
79/190 train_loss: 0.781602, train_acc: 65.976562
99/190 train_loss: 0.779053, train_acc: 66.406250
119/190 train_loss: 0.779031, train_acc: 66.067708
139/190 train_loss: 0.767833, train_acc: 66.852679
159/190 train_loss: 0.765592, train_acc: 66.972656
179/190 train_loss: 0.763588, train_acc: 66.770833
End of 5 train_loss: 0.7616, train_acc: 66.8750, val_loss: 0.7501, val_acc: 70.3977, f1_score: 0.6826
new best model saved.
Epoch 6------------------------------------------------------------
19/190 train_loss: 0.797663, train_acc: 64.062500
39/190 train_loss: 0.789859, train_acc: 65.312500
59/190 train_loss: 0.783242, train_acc: 64.583333
79/190 train_loss: 0.772993, train_acc: 65.664062
99/190 train_loss: 0.769195, train_acc: 66.062500
119/190 train_loss: 0.770854, train_acc: 65.989583
139/190 train_loss: 0.759344, train_acc: 66.830357
159/190 train_loss: 0.759983, train_acc: 66.875000
179/190 train_loss: 0.755818, train_acc: 67.048611
End of 6 train_loss: 0.7538, train_acc: 67.1930, val_loss: 0.7543, val_acc: 69.4034, f1_score: 0.6788
Epoch 7------------------------------------------------------------
19/190 train_loss: 0.752167, train_acc: 66.875000
39/190 train_loss: 0.764767, train_acc: 66.015625
59/190 train_loss: 0.754250, train_acc: 66.822917
79/190 train_loss: 0.738803, train_acc: 67.539062
99/190 train_loss: 0.730850, train_acc: 68.093750
119/190 train_loss: 0.734692, train_acc: 67.630208
139/190 train_loss: 0.730167, train_acc: 68.080357
159/190 train_loss: 0.725807, train_acc: 68.378906
179/190 train_loss: 0.721653, train_acc: 68.663194
End of 7 train_loss: 0.7192, train_acc: 68.6404, val_loss: 0.7590, val_acc: 71.5341, f1_score: 0.7021
new best model saved.
Epoch 8------------------------------------------------------------
19/190 train_loss: 0.717857, train_acc: 69.218750
39/190 train_loss: 0.739232, train_acc: 67.890625
59/190 train_loss: 0.743401, train_acc: 67.864583
79/190 train_loss: 0.731936, train_acc: 68.593750
99/190 train_loss: 0.721830, train_acc: 69.187500
119/190 train_loss: 0.724380, train_acc: 69.062500
139/190 train_loss: 0.712755, train_acc: 69.799107
159/190 train_loss: 0.709300, train_acc: 70.078125
179/190 train_loss: 0.706158, train_acc: 69.930556
End of 8 train_loss: 0.7043, train_acc: 70.0055, val_loss: 0.7099, val_acc: 72.1023, f1_score: 0.7048
new best model saved.
Epoch 9------------------------------------------------------------
19/190 train_loss: 0.748600, train_acc: 67.187500
39/190 train_loss: 0.746968, train_acc: 67.421875
59/190 train_loss: 0.742779, train_acc: 67.916667
79/190 train_loss: 0.734008, train_acc: 68.593750
99/190 train_loss: 0.720657, train_acc: 69.531250
119/190 train_loss: 0.723101, train_acc: 68.776042
139/190 train_loss: 0.715252, train_acc: 69.040179
159/190 train_loss: 0.711791, train_acc: 69.140625
179/190 train_loss: 0.709950, train_acc: 69.409722
End of 9 train_loss: 0.7081, train_acc: 69.5230, val_loss: 0.7096, val_acc: 71.1080, f1_score: 0.6941
Epoch 10------------------------------------------------------------
19/190 train_loss: 0.748397, train_acc: 67.031250
39/190 train_loss: 0.742328, train_acc: 66.328125
59/190 train_loss: 0.735775, train_acc: 66.666667
79/190 train_loss: 0.724919, train_acc: 67.382812
99/190 train_loss: 0.713476, train_acc: 68.625000
119/190 train_loss: 0.713782, train_acc: 68.489583
139/190 train_loss: 0.703605, train_acc: 69.263393
159/190 train_loss: 0.698148, train_acc: 69.863281
179/190 train_loss: 0.694112, train_acc: 69.895833
End of 10 train_loss: 0.6894, train_acc: 70.0164, val_loss: 0.7217, val_acc: 70.2557, f1_score: 0.6872
Epoch 11------------------------------------------------------------
19/190 train_loss: 0.724792, train_acc: 67.656250
39/190 train_loss: 0.732683, train_acc: 66.796875
59/190 train_loss: 0.726062, train_acc: 67.187500
79/190 train_loss: 0.714563, train_acc: 67.812500
99/190 train_loss: 0.700065, train_acc: 69.031250
119/190 train_loss: 0.696432, train_acc: 69.322917
139/190 train_loss: 0.688297, train_acc: 70.066964
159/190 train_loss: 0.686199, train_acc: 70.429688
179/190 train_loss: 0.682159, train_acc: 70.729167
End of 11 train_loss: 0.6818, train_acc: 70.8388, val_loss: 0.7221, val_acc: 70.1136, f1_score: 0.6826
Epoch 12------------------------------------------------------------
19/190 train_loss: 0.691043, train_acc: 68.906250
39/190 train_loss: 0.708129, train_acc: 68.125000
59/190 train_loss: 0.707875, train_acc: 68.489583
79/190 train_loss: 0.697304, train_acc: 68.906250
99/190 train_loss: 0.694129, train_acc: 69.343750
119/190 train_loss: 0.696388, train_acc: 69.270833
139/190 train_loss: 0.685797, train_acc: 70.156250
159/190 train_loss: 0.681360, train_acc: 70.625000
179/190 train_loss: 0.679428, train_acc: 70.833333
End of 12 train_loss: 0.6752, train_acc: 71.1184, val_loss: 0.7302, val_acc: 70.5398, f1_score: 0.6917
Epoch 13------------------------------------------------------------
19/190 train_loss: 0.691604, train_acc: 70.781250
39/190 train_loss: 0.699886, train_acc: 70.078125
59/190 train_loss: 0.698887, train_acc: 70.520833
79/190 train_loss: 0.690743, train_acc: 70.859375
99/190 train_loss: 0.676808, train_acc: 71.250000
119/190 train_loss: 0.680446, train_acc: 70.755208
139/190 train_loss: 0.673691, train_acc: 70.915179
159/190 train_loss: 0.671392, train_acc: 71.015625
179/190 train_loss: 0.669711, train_acc: 71.128472
End of 13 train_loss: 0.6686, train_acc: 71.1678, val_loss: 0.7021, val_acc: 71.9602, f1_score: 0.7051
Epoch 14------------------------------------------------------------
19/190 train_loss: 0.676699, train_acc: 70.625000
39/190 train_loss: 0.679947, train_acc: 70.000000
59/190 train_loss: 0.679796, train_acc: 70.677083
79/190 train_loss: 0.671542, train_acc: 71.093750
99/190 train_loss: 0.661752, train_acc: 71.000000
119/190 train_loss: 0.669947, train_acc: 70.625000
139/190 train_loss: 0.659977, train_acc: 71.316964
159/190 train_loss: 0.662031, train_acc: 71.464844
179/190 train_loss: 0.658407, train_acc: 71.770833
End of 14 train_loss: 0.6553, train_acc: 71.9408, val_loss: 0.7167, val_acc: 71.8182, f1_score: 0.7051
Epoch 15------------------------------------------------------------
19/190 train_loss: 0.674040, train_acc: 71.093750
39/190 train_loss: 0.674771, train_acc: 71.562500
59/190 train_loss: 0.671137, train_acc: 71.354167
79/190 train_loss: 0.668570, train_acc: 71.210938
99/190 train_loss: 0.650121, train_acc: 72.218750
119/190 train_loss: 0.652043, train_acc: 71.875000
139/190 train_loss: 0.644811, train_acc: 72.477679
159/190 train_loss: 0.641898, train_acc: 72.539062
179/190 train_loss: 0.642513, train_acc: 72.586806
End of 15 train_loss: 0.6410, train_acc: 72.5877, val_loss: 0.7325, val_acc: 72.2443, f1_score: 0.7120
new best model saved.
Epoch 16------------------------------------------------------------
19/190 train_loss: 0.687977, train_acc: 70.625000
39/190 train_loss: 0.684503, train_acc: 70.078125
59/190 train_loss: 0.681487, train_acc: 70.156250
79/190 train_loss: 0.672430, train_acc: 71.054688
99/190 train_loss: 0.659846, train_acc: 71.531250
119/190 train_loss: 0.662221, train_acc: 71.223958
139/190 train_loss: 0.651009, train_acc: 71.897321
159/190 train_loss: 0.645266, train_acc: 72.226562
179/190 train_loss: 0.640794, train_acc: 72.413194
End of 16 train_loss: 0.6369, train_acc: 72.6316, val_loss: 0.7185, val_acc: 70.6818, f1_score: 0.6909
Epoch 17------------------------------------------------------------
19/190 train_loss: 0.690485, train_acc: 70.468750
39/190 train_loss: 0.697180, train_acc: 69.296875
59/190 train_loss: 0.689441, train_acc: 70.000000
79/190 train_loss: 0.669541, train_acc: 70.859375
99/190 train_loss: 0.661294, train_acc: 71.125000
119/190 train_loss: 0.662601, train_acc: 71.041667
139/190 train_loss: 0.649971, train_acc: 71.941964
159/190 train_loss: 0.649045, train_acc: 72.070312
179/190 train_loss: 0.645732, train_acc: 72.135417
End of 17 train_loss: 0.6425, train_acc: 72.2204, val_loss: 0.7332, val_acc: 72.8125, f1_score: 0.7127
new best model saved.
Epoch 18------------------------------------------------------------
19/190 train_loss: 0.648168, train_acc: 71.093750
39/190 train_loss: 0.653306, train_acc: 70.859375
59/190 train_loss: 0.659431, train_acc: 70.729167
79/190 train_loss: 0.654784, train_acc: 71.289062
99/190 train_loss: 0.647780, train_acc: 71.687500
119/190 train_loss: 0.647671, train_acc: 71.692708
139/190 train_loss: 0.640773, train_acc: 72.299107
159/190 train_loss: 0.639131, train_acc: 72.460938
179/190 train_loss: 0.636481, train_acc: 72.777778
End of 18 train_loss: 0.6324, train_acc: 72.8618, val_loss: 0.7463, val_acc: 70.8239, f1_score: 0.6941
Epoch 19------------------------------------------------------------
19/190 train_loss: 0.660598, train_acc: 73.593750
39/190 train_loss: 0.669327, train_acc: 72.812500
59/190 train_loss: 0.664700, train_acc: 72.187500
79/190 train_loss: 0.649516, train_acc: 72.578125
99/190 train_loss: 0.638920, train_acc: 73.000000
119/190 train_loss: 0.641824, train_acc: 72.526042
139/190 train_loss: 0.631536, train_acc: 73.281250
159/190 train_loss: 0.627828, train_acc: 73.574219
179/190 train_loss: 0.625890, train_acc: 73.611111
End of 19 train_loss: 0.6235, train_acc: 73.8322, val_loss: 0.7173, val_acc: 72.2443, f1_score: 0.7098
Epoch 20------------------------------------------------------------
19/190 train_loss: 0.657133, train_acc: 71.250000
39/190 train_loss: 0.638499, train_acc: 72.109375
59/190 train_loss: 0.634170, train_acc: 72.343750
79/190 train_loss: 0.631225, train_acc: 72.421875
99/190 train_loss: 0.626677, train_acc: 72.718750
119/190 train_loss: 0.622134, train_acc: 72.916667
139/190 train_loss: 0.617865, train_acc: 73.504464
159/190 train_loss: 0.619126, train_acc: 73.476562
179/190 train_loss: 0.617712, train_acc: 73.472222
End of 20 train_loss: 0.6167, train_acc: 73.6184, val_loss: 0.7125, val_acc: 72.5284, f1_score: 0.7096
Epoch 21------------------------------------------------------------
19/190 train_loss: 0.659048, train_acc: 70.937500
39/190 train_loss: 0.644650, train_acc: 73.750000
59/190 train_loss: 0.643606, train_acc: 73.177083
79/190 train_loss: 0.632522, train_acc: 73.632812
99/190 train_loss: 0.628287, train_acc: 73.343750
119/190 train_loss: 0.632303, train_acc: 73.333333
139/190 train_loss: 0.624001, train_acc: 73.906250
159/190 train_loss: 0.620761, train_acc: 74.042969
179/190 train_loss: 0.616713, train_acc: 74.288194
End of 21 train_loss: 0.6125, train_acc: 74.4408, val_loss: 0.7099, val_acc: 73.2386, f1_score: 0.7204
new best model saved.
Epoch 22------------------------------------------------------------
19/190 train_loss: 0.613146, train_acc: 72.500000
39/190 train_loss: 0.643759, train_acc: 72.031250
59/190 train_loss: 0.645510, train_acc: 71.614583
79/190 train_loss: 0.637271, train_acc: 71.757812
99/190 train_loss: 0.628889, train_acc: 72.218750
119/190 train_loss: 0.631467, train_acc: 72.447917
139/190 train_loss: 0.619885, train_acc: 73.303571
159/190 train_loss: 0.616215, train_acc: 73.750000
179/190 train_loss: 0.611403, train_acc: 74.166667
End of 22 train_loss: 0.6060, train_acc: 74.4408, val_loss: 0.7389, val_acc: 72.8125, f1_score: 0.7161
Epoch 23------------------------------------------------------------
19/190 train_loss: 0.620523, train_acc: 72.968750
39/190 train_loss: 0.623441, train_acc: 73.125000
59/190 train_loss: 0.622876, train_acc: 73.645833
79/190 train_loss: 0.614813, train_acc: 74.179688
99/190 train_loss: 0.602564, train_acc: 75.000000
119/190 train_loss: 0.605811, train_acc: 74.505208
139/190 train_loss: 0.603834, train_acc: 74.575893
159/190 train_loss: 0.600843, train_acc: 74.609375
179/190 train_loss: 0.597866, train_acc: 74.739583
End of 23 train_loss: 0.5959, train_acc: 74.7094, val_loss: 0.7077, val_acc: 73.5227, f1_score: 0.7222
new best model saved.
Epoch 24------------------------------------------------------------
19/190 train_loss: 0.639454, train_acc: 72.343750
39/190 train_loss: 0.633716, train_acc: 73.125000
59/190 train_loss: 0.620773, train_acc: 73.437500
79/190 train_loss: 0.616573, train_acc: 73.906250
99/190 train_loss: 0.611821, train_acc: 73.906250
119/190 train_loss: 0.608660, train_acc: 74.010417
139/190 train_loss: 0.599135, train_acc: 74.575893
159/190 train_loss: 0.595614, train_acc: 74.843750
179/190 train_loss: 0.589754, train_acc: 75.069444
End of 24 train_loss: 0.5886, train_acc: 74.9561, val_loss: 0.7124, val_acc: 73.5227, f1_score: 0.7210
Epoch 25------------------------------------------------------------
19/190 train_loss: 0.618046, train_acc: 72.031250
39/190 train_loss: 0.627277, train_acc: 73.125000
59/190 train_loss: 0.617749, train_acc: 73.385417
79/190 train_loss: 0.602014, train_acc: 73.906250
99/190 train_loss: 0.598227, train_acc: 74.187500
119/190 train_loss: 0.607152, train_acc: 73.906250
139/190 train_loss: 0.601539, train_acc: 74.352679
159/190 train_loss: 0.601922, train_acc: 74.609375
179/190 train_loss: 0.598808, train_acc: 74.861111
End of 25 train_loss: 0.5976, train_acc: 74.9178, val_loss: 0.6912, val_acc: 75.3693, f1_score: 0.7415
new best model saved.
Epoch 26------------------------------------------------------------
19/190 train_loss: 0.637030, train_acc: 71.718750
39/190 train_loss: 0.626088, train_acc: 72.812500
59/190 train_loss: 0.618897, train_acc: 72.864583
79/190 train_loss: 0.612136, train_acc: 73.593750
99/190 train_loss: 0.605234, train_acc: 74.187500
119/190 train_loss: 0.609426, train_acc: 74.244792
139/190 train_loss: 0.600491, train_acc: 74.665179
159/190 train_loss: 0.601193, train_acc: 74.785156
179/190 train_loss: 0.599324, train_acc: 74.930556
End of 26 train_loss: 0.5969, train_acc: 75.1151, val_loss: 0.7359, val_acc: 73.3807, f1_score: 0.7199
Epoch 27------------------------------------------------------------
19/190 train_loss: 0.638210, train_acc: 71.718750
39/190 train_loss: 0.623534, train_acc: 72.734375
59/190 train_loss: 0.618593, train_acc: 72.968750
79/190 train_loss: 0.608211, train_acc: 73.281250
99/190 train_loss: 0.598646, train_acc: 73.906250
119/190 train_loss: 0.597128, train_acc: 74.270833
139/190 train_loss: 0.591571, train_acc: 74.575893
159/190 train_loss: 0.589963, train_acc: 74.765625
179/190 train_loss: 0.587191, train_acc: 74.895833
End of 27 train_loss: 0.5844, train_acc: 75.0329, val_loss: 0.7637, val_acc: 71.2500, f1_score: 0.6965
Epoch 28------------------------------------------------------------
19/190 train_loss: 0.625291, train_acc: 72.187500
39/190 train_loss: 0.617737, train_acc: 72.656250
59/190 train_loss: 0.619105, train_acc: 72.656250
79/190 train_loss: 0.615204, train_acc: 72.929688
99/190 train_loss: 0.610177, train_acc: 73.187500
119/190 train_loss: 0.603207, train_acc: 73.541667
139/190 train_loss: 0.594652, train_acc: 74.129464
159/190 train_loss: 0.586009, train_acc: 74.824219
179/190 train_loss: 0.580651, train_acc: 75.121528
End of 28 train_loss: 0.5784, train_acc: 75.1645, val_loss: 0.7283, val_acc: 73.0966, f1_score: 0.7200
Epoch 29------------------------------------------------------------
19/190 train_loss: 0.584966, train_acc: 74.375000
39/190 train_loss: 0.598315, train_acc: 74.062500
59/190 train_loss: 0.601245, train_acc: 74.114583
79/190 train_loss: 0.593673, train_acc: 74.335938
99/190 train_loss: 0.588394, train_acc: 74.562500
119/190 train_loss: 0.593688, train_acc: 74.401042
139/190 train_loss: 0.586303, train_acc: 74.910714
159/190 train_loss: 0.583511, train_acc: 75.195312
179/190 train_loss: 0.581680, train_acc: 75.173611
End of 29 train_loss: 0.5807, train_acc: 75.0713, val_loss: 0.7098, val_acc: 73.6648, f1_score: 0.7246
Epoch 30------------------------------------------------------------
19/190 train_loss: 0.647234, train_acc: 73.125000
39/190 train_loss: 0.632290, train_acc: 74.062500
59/190 train_loss: 0.610111, train_acc: 74.218750
79/190 train_loss: 0.609451, train_acc: 73.945312
99/190 train_loss: 0.601893, train_acc: 74.156250
119/190 train_loss: 0.605631, train_acc: 74.192708
139/190 train_loss: 0.599441, train_acc: 74.575893
159/190 train_loss: 0.598040, train_acc: 74.726562
179/190 train_loss: 0.593129, train_acc: 75.000000
End of 30 train_loss: 0.5895, train_acc: 75.2138, val_loss: 0.7333, val_acc: 72.3864, f1_score: 0.7121
Epoch 31------------------------------------------------------------
19/190 train_loss: 0.590298, train_acc: 75.468750
39/190 train_loss: 0.603397, train_acc: 73.281250
59/190 train_loss: 0.604117, train_acc: 73.437500
79/190 train_loss: 0.596079, train_acc: 74.101562
99/190 train_loss: 0.579977, train_acc: 74.812500
119/190 train_loss: 0.588066, train_acc: 74.375000
139/190 train_loss: 0.583523, train_acc: 74.642857
159/190 train_loss: 0.578803, train_acc: 75.019531
179/190 train_loss: 0.573918, train_acc: 75.312500
End of 31 train_loss: 0.5725, train_acc: 75.4441, val_loss: 0.7067, val_acc: 73.5227, f1_score: 0.7228
Epoch 32------------------------------------------------------------
19/190 train_loss: 0.624228, train_acc: 73.125000
39/190 train_loss: 0.603813, train_acc: 73.984375
59/190 train_loss: 0.605237, train_acc: 73.802083
79/190 train_loss: 0.600390, train_acc: 73.632812
99/190 train_loss: 0.587122, train_acc: 74.781250
119/190 train_loss: 0.587244, train_acc: 74.869792
139/190 train_loss: 0.579176, train_acc: 75.424107
159/190 train_loss: 0.572241, train_acc: 75.566406
179/190 train_loss: 0.569661, train_acc: 75.781250
End of 32 train_loss: 0.5667, train_acc: 75.9539, val_loss: 0.7327, val_acc: 73.3807, f1_score: 0.7235
Epoch 33------------------------------------------------------------
19/190 train_loss: 0.553065, train_acc: 78.750000
39/190 train_loss: 0.570853, train_acc: 77.109375
59/190 train_loss: 0.582086, train_acc: 76.250000
79/190 train_loss: 0.585103, train_acc: 75.820312
99/190 train_loss: 0.574094, train_acc: 76.156250
119/190 train_loss: 0.581526, train_acc: 75.755208
139/190 train_loss: 0.574445, train_acc: 76.183036
159/190 train_loss: 0.571214, train_acc: 76.210938
179/190 train_loss: 0.568722, train_acc: 76.406250
End of 33 train_loss: 0.5652, train_acc: 76.3871, val_loss: 0.7736, val_acc: 72.5284, f1_score: 0.7121
Epoch 34------------------------------------------------------------
19/190 train_loss: 0.574139, train_acc: 75.625000
39/190 train_loss: 0.578567, train_acc: 74.921875
59/190 train_loss: 0.575519, train_acc: 75.312500
79/190 train_loss: 0.570652, train_acc: 75.742188
99/190 train_loss: 0.562545, train_acc: 76.468750
119/190 train_loss: 0.573820, train_acc: 76.067708
139/190 train_loss: 0.566307, train_acc: 76.517857
159/190 train_loss: 0.562698, train_acc: 76.933594
179/190 train_loss: 0.557343, train_acc: 77.031250
End of 34 train_loss: 0.5534, train_acc: 77.0614, val_loss: 0.7443, val_acc: 75.0852, f1_score: 0.7382
Epoch 35------------------------------------------------------------
19/190 train_loss: 0.593105, train_acc: 75.156250
39/190 train_loss: 0.587685, train_acc: 75.937500
59/190 train_loss: 0.586263, train_acc: 75.104167
79/190 train_loss: 0.575701, train_acc: 75.664062
99/190 train_loss: 0.565007, train_acc: 76.312500
119/190 train_loss: 0.568940, train_acc: 76.119792
139/190 train_loss: 0.561266, train_acc: 76.540179
159/190 train_loss: 0.558050, train_acc: 76.640625
179/190 train_loss: 0.555993, train_acc: 76.631944
End of 35 train_loss: 0.5572, train_acc: 76.6283, val_loss: 0.7272, val_acc: 73.8068, f1_score: 0.7228
Epoch 36------------------------------------------------------------
19/190 train_loss: 0.545715, train_acc: 76.875000
39/190 train_loss: 0.557331, train_acc: 75.468750
59/190 train_loss: 0.567917, train_acc: 75.572917
79/190 train_loss: 0.554171, train_acc: 76.015625
99/190 train_loss: 0.549394, train_acc: 76.562500
119/190 train_loss: 0.548163, train_acc: 76.536458
139/190 train_loss: 0.542029, train_acc: 76.919643
159/190 train_loss: 0.540825, train_acc: 77.226562
179/190 train_loss: 0.541546, train_acc: 77.361111
End of 36 train_loss: 0.5379, train_acc: 77.5164, val_loss: 0.7429, val_acc: 73.2386, f1_score: 0.7214
Epoch 37------------------------------------------------------------
19/190 train_loss: 0.565863, train_acc: 76.718750
39/190 train_loss: 0.573139, train_acc: 76.718750
59/190 train_loss: 0.577938, train_acc: 75.885417
79/190 train_loss: 0.568160, train_acc: 76.210938
99/190 train_loss: 0.552846, train_acc: 76.906250
119/190 train_loss: 0.556646, train_acc: 76.796875
139/190 train_loss: 0.549943, train_acc: 77.187500
159/190 train_loss: 0.546046, train_acc: 77.480469
179/190 train_loss: 0.542276, train_acc: 77.604167
End of 37 train_loss: 0.5401, train_acc: 77.6645, val_loss: 0.7840, val_acc: 73.6648, f1_score: 0.7203
Epoch 38------------------------------------------------------------
19/190 train_loss: 0.587678, train_acc: 75.781250
39/190 train_loss: 0.581537, train_acc: 76.484375
59/190 train_loss: 0.575795, train_acc: 76.197917
79/190 train_loss: 0.568356, train_acc: 76.445312
99/190 train_loss: 0.563214, train_acc: 76.718750
119/190 train_loss: 0.563650, train_acc: 76.770833
139/190 train_loss: 0.557693, train_acc: 77.053571
159/190 train_loss: 0.549665, train_acc: 77.539062
179/190 train_loss: 0.544351, train_acc: 77.690972
End of 38 train_loss: 0.5423, train_acc: 77.7303, val_loss: 0.7774, val_acc: 73.3807, f1_score: 0.7190
Epoch 39------------------------------------------------------------
19/190 train_loss: 0.560676, train_acc: 77.031250
39/190 train_loss: 0.543006, train_acc: 77.109375
59/190 train_loss: 0.558597, train_acc: 76.562500
79/190 train_loss: 0.548038, train_acc: 76.875000
99/190 train_loss: 0.544900, train_acc: 76.968750
119/190 train_loss: 0.542964, train_acc: 77.161458
139/190 train_loss: 0.535678, train_acc: 77.544643
159/190 train_loss: 0.532811, train_acc: 77.675781
179/190 train_loss: 0.529358, train_acc: 78.020833
End of 39 train_loss: 0.5252, train_acc: 78.2895, val_loss: 0.7870, val_acc: 73.2386, f1_score: 0.7150
Epoch 40------------------------------------------------------------
19/190 train_loss: 0.548391, train_acc: 77.187500
39/190 train_loss: 0.566136, train_acc: 76.953125
59/190 train_loss: 0.568096, train_acc: 76.562500
79/190 train_loss: 0.558430, train_acc: 76.367188
99/190 train_loss: 0.552251, train_acc: 76.812500
119/190 train_loss: 0.551349, train_acc: 76.796875
139/190 train_loss: 0.545343, train_acc: 77.053571
159/190 train_loss: 0.539109, train_acc: 77.363281
179/190 train_loss: 0.537977, train_acc: 77.395833
End of 40 train_loss: 0.5335, train_acc: 77.5658, val_loss: 0.8204, val_acc: 72.1875, f1_score: 0.7151
Epoch 41------------------------------------------------------------
19/190 train_loss: 0.542317, train_acc: 77.968750
39/190 train_loss: 0.562641, train_acc: 76.171875
59/190 train_loss: 0.564883, train_acc: 75.989583
79/190 train_loss: 0.569170, train_acc: 76.132812
99/190 train_loss: 0.567221, train_acc: 75.875000
119/190 train_loss: 0.567153, train_acc: 76.093750
139/190 train_loss: 0.559532, train_acc: 76.629464
159/190 train_loss: 0.549085, train_acc: 77.246094
179/190 train_loss: 0.544270, train_acc: 77.604167
End of 41 train_loss: 0.5374, train_acc: 77.8783, val_loss: 0.8372, val_acc: 73.3807, f1_score: 0.7203
Epoch 42------------------------------------------------------------
19/190 train_loss: 0.528093, train_acc: 78.437500
39/190 train_loss: 0.560610, train_acc: 77.812500
59/190 train_loss: 0.551147, train_acc: 77.552083
79/190 train_loss: 0.550044, train_acc: 77.187500
99/190 train_loss: 0.542465, train_acc: 77.406250
119/190 train_loss: 0.543932, train_acc: 77.317708
139/190 train_loss: 0.533051, train_acc: 77.924107
159/190 train_loss: 0.530470, train_acc: 78.281250
179/190 train_loss: 0.524413, train_acc: 78.593750
End of 42 train_loss: 0.5211, train_acc: 78.5526, val_loss: 0.7944, val_acc: 71.8182, f1_score: 0.7046
Epoch 43------------------------------------------------------------
19/190 train_loss: 0.552174, train_acc: 75.156250
39/190 train_loss: 0.554899, train_acc: 75.390625
59/190 train_loss: 0.558983, train_acc: 75.520833
79/190 train_loss: 0.548515, train_acc: 76.093750
99/190 train_loss: 0.537138, train_acc: 76.750000
119/190 train_loss: 0.536042, train_acc: 76.927083
139/190 train_loss: 0.527632, train_acc: 77.544643
159/190 train_loss: 0.524762, train_acc: 77.871094
179/190 train_loss: 0.522332, train_acc: 77.934028
End of 43 train_loss: 0.5193, train_acc: 77.9934, val_loss: 0.7910, val_acc: 72.6705, f1_score: 0.7121
Epoch 44------------------------------------------------------------
19/190 train_loss: 0.526844, train_acc: 77.812500
39/190 train_loss: 0.529425, train_acc: 78.906250
59/190 train_loss: 0.533388, train_acc: 78.541667
79/190 train_loss: 0.532998, train_acc: 78.515625
99/190 train_loss: 0.515771, train_acc: 79.375000
119/190 train_loss: 0.519107, train_acc: 79.427083
139/190 train_loss: 0.510359, train_acc: 79.665179
159/190 train_loss: 0.506196, train_acc: 79.882812
179/190 train_loss: 0.505963, train_acc: 79.722222
End of 44 train_loss: 0.5040, train_acc: 79.7039, val_loss: 0.8045, val_acc: 70.9659, f1_score: 0.6926
Epoch 45------------------------------------------------------------
19/190 train_loss: 0.504002, train_acc: 81.093750
39/190 train_loss: 0.522298, train_acc: 79.531250
59/190 train_loss: 0.543691, train_acc: 78.333333
79/190 train_loss: 0.540408, train_acc: 78.476562
99/190 train_loss: 0.528315, train_acc: 78.968750
119/190 train_loss: 0.529594, train_acc: 78.802083
139/190 train_loss: 0.518697, train_acc: 79.241071
159/190 train_loss: 0.515569, train_acc: 79.394531
179/190 train_loss: 0.507957, train_acc: 79.531250
End of 45 train_loss: 0.5066, train_acc: 79.4792, val_loss: 0.8297, val_acc: 72.3864, f1_score: 0.7114
Epoch 46------------------------------------------------------------
19/190 train_loss: 0.502199, train_acc: 78.281250
39/190 train_loss: 0.528152, train_acc: 77.890625
59/190 train_loss: 0.525374, train_acc: 78.125000
79/190 train_loss: 0.511927, train_acc: 78.359375
99/190 train_loss: 0.503328, train_acc: 78.906250
119/190 train_loss: 0.508632, train_acc: 78.750000
139/190 train_loss: 0.503476, train_acc: 79.062500
159/190 train_loss: 0.497973, train_acc: 79.433594
179/190 train_loss: 0.494055, train_acc: 79.722222
End of 46 train_loss: 0.4894, train_acc: 79.8849, val_loss: 0.8395, val_acc: 71.3920, f1_score: 0.6995
Epoch 47------------------------------------------------------------
19/190 train_loss: 0.511989, train_acc: 79.062500
39/190 train_loss: 0.533416, train_acc: 77.343750
59/190 train_loss: 0.546816, train_acc: 76.927083
79/190 train_loss: 0.541363, train_acc: 77.187500
99/190 train_loss: 0.530279, train_acc: 77.593750
119/190 train_loss: 0.532518, train_acc: 77.734375
139/190 train_loss: 0.522723, train_acc: 78.147321
159/190 train_loss: 0.515259, train_acc: 78.691406
179/190 train_loss: 0.513832, train_acc: 78.663194
End of 47 train_loss: 0.5083, train_acc: 78.9145, val_loss: 0.7849, val_acc: 71.3920, f1_score: 0.7007
Epoch 48------------------------------------------------------------
19/190 train_loss: 0.521837, train_acc: 77.968750
39/190 train_loss: 0.510723, train_acc: 78.046875
59/190 train_loss: 0.529648, train_acc: 78.072917
79/190 train_loss: 0.522156, train_acc: 78.085938
99/190 train_loss: 0.510524, train_acc: 78.656250
119/190 train_loss: 0.503661, train_acc: 78.984375
139/190 train_loss: 0.497974, train_acc: 79.196429
159/190 train_loss: 0.498818, train_acc: 79.179688
179/190 train_loss: 0.494366, train_acc: 79.461806
End of 48 train_loss: 0.4905, train_acc: 79.5559, val_loss: 0.8223, val_acc: 71.5341, f1_score: 0.7019
Epoch 49------------------------------------------------------------
19/190 train_loss: 0.514425, train_acc: 80.312500
39/190 train_loss: 0.515554, train_acc: 79.531250
59/190 train_loss: 0.523716, train_acc: 78.697917
79/190 train_loss: 0.519277, train_acc: 78.828125
99/190 train_loss: 0.514613, train_acc: 78.781250
119/190 train_loss: 0.514808, train_acc: 78.932292
139/190 train_loss: 0.511480, train_acc: 79.017857
159/190 train_loss: 0.512042, train_acc: 79.023438
179/190 train_loss: 0.509075, train_acc: 79.201389
End of 49 train_loss: 0.5061, train_acc: 79.3750, val_loss: 0.8223, val_acc: 72.1023, f1_score: 0.7100
Epoch 50------------------------------------------------------------
19/190 train_loss: 0.503231, train_acc: 78.281250
39/190 train_loss: 0.516511, train_acc: 79.140625
59/190 train_loss: 0.519369, train_acc: 78.489583
79/190 train_loss: 0.515530, train_acc: 78.945312
99/190 train_loss: 0.498158, train_acc: 79.437500
119/190 train_loss: 0.500203, train_acc: 79.427083
139/190 train_loss: 0.497611, train_acc: 79.508929
159/190 train_loss: 0.501148, train_acc: 79.335938
179/190 train_loss: 0.497216, train_acc: 79.496528
End of 50 train_loss: 0.4939, train_acc: 79.5559, val_loss: 0.8184, val_acc: 72.1023, f1_score: 0.7065
Epoch 51------------------------------------------------------------
19/190 train_loss: 0.492829, train_acc: 80.156250
39/190 train_loss: 0.499879, train_acc: 79.843750
59/190 train_loss: 0.513780, train_acc: 78.177083
79/190 train_loss: 0.513608, train_acc: 77.773438
99/190 train_loss: 0.504797, train_acc: 77.937500
119/190 train_loss: 0.505456, train_acc: 78.098958
139/190 train_loss: 0.499290, train_acc: 78.571429
159/190 train_loss: 0.500971, train_acc: 78.710938
179/190 train_loss: 0.496678, train_acc: 79.027778
End of 51 train_loss: 0.4945, train_acc: 79.1612, val_loss: 0.8412, val_acc: 72.1023, f1_score: 0.7088
Epoch 52------------------------------------------------------------
19/190 train_loss: 0.461923, train_acc: 82.031250
39/190 train_loss: 0.502916, train_acc: 79.921875
59/190 train_loss: 0.517287, train_acc: 78.958333
79/190 train_loss: 0.520854, train_acc: 78.710938
99/190 train_loss: 0.509233, train_acc: 79.156250
119/190 train_loss: 0.510264, train_acc: 79.192708
139/190 train_loss: 0.498570, train_acc: 79.575893
159/190 train_loss: 0.498155, train_acc: 79.589844
179/190 train_loss: 0.497970, train_acc: 79.427083
End of 52 train_loss: 0.4970, train_acc: 79.6053, val_loss: 0.8100, val_acc: 72.6705, f1_score: 0.7140
Epoch 53------------------------------------------------------------
19/190 train_loss: 0.499033, train_acc: 79.843750
39/190 train_loss: 0.496592, train_acc: 79.687500
59/190 train_loss: 0.508200, train_acc: 79.270833
79/190 train_loss: 0.503007, train_acc: 79.453125
99/190 train_loss: 0.499206, train_acc: 79.250000
119/190 train_loss: 0.503378, train_acc: 79.036458
139/190 train_loss: 0.497128, train_acc: 79.285714
159/190 train_loss: 0.491234, train_acc: 79.414062
179/190 train_loss: 0.490081, train_acc: 79.375000
End of 53 train_loss: 0.4836, train_acc: 79.7533, val_loss: 0.8287, val_acc: 71.2500, f1_score: 0.7013
Epoch 54------------------------------------------------------------
19/190 train_loss: 0.514252, train_acc: 78.281250
39/190 train_loss: 0.508398, train_acc: 78.125000
59/190 train_loss: 0.518784, train_acc: 78.072917
79/190 train_loss: 0.512171, train_acc: 78.242188
99/190 train_loss: 0.507513, train_acc: 78.312500
119/190 train_loss: 0.510941, train_acc: 77.994792
139/190 train_loss: 0.501429, train_acc: 78.437500
159/190 train_loss: 0.498947, train_acc: 78.769531
179/190 train_loss: 0.500050, train_acc: 78.802083
End of 54 train_loss: 0.4933, train_acc: 79.2599, val_loss: 0.8001, val_acc: 71.5341, f1_score: 0.7018
Epoch 55------------------------------------------------------------
19/190 train_loss: 0.497822, train_acc: 78.593750
39/190 train_loss: 0.503296, train_acc: 77.734375
59/190 train_loss: 0.504684, train_acc: 78.177083
79/190 train_loss: 0.502650, train_acc: 78.242188
99/190 train_loss: 0.494394, train_acc: 78.968750
119/190 train_loss: 0.494581, train_acc: 79.088542
139/190 train_loss: 0.487154, train_acc: 79.375000
159/190 train_loss: 0.486741, train_acc: 79.238281
179/190 train_loss: 0.484830, train_acc: 79.270833
End of 55 train_loss: 0.4815, train_acc: 79.4737, val_loss: 0.8187, val_acc: 71.3920, f1_score: 0.7010
Epoch 56------------------------------------------------------------
19/190 train_loss: 0.500625, train_acc: 78.750000
39/190 train_loss: 0.495663, train_acc: 79.218750
59/190 train_loss: 0.492006, train_acc: 79.479167
79/190 train_loss: 0.490882, train_acc: 79.726562
99/190 train_loss: 0.484023, train_acc: 80.250000
119/190 train_loss: 0.483207, train_acc: 80.156250
139/190 train_loss: 0.480272, train_acc: 80.044643
159/190 train_loss: 0.483840, train_acc: 80.039062
179/190 train_loss: 0.485344, train_acc: 80.034722
End of 56 train_loss: 0.4819, train_acc: 80.2632, val_loss: 0.8152, val_acc: 71.9602, f1_score: 0.7064
Epoch 57------------------------------------------------------------
19/190 train_loss: 0.485746, train_acc: 80.156250
39/190 train_loss: 0.495403, train_acc: 79.765625
59/190 train_loss: 0.489470, train_acc: 79.531250
79/190 train_loss: 0.488375, train_acc: 79.453125
99/190 train_loss: 0.476583, train_acc: 80.062500
119/190 train_loss: 0.470702, train_acc: 80.416667
139/190 train_loss: 0.467544, train_acc: 80.625000
159/190 train_loss: 0.469796, train_acc: 80.761719
179/190 train_loss: 0.468063, train_acc: 80.729167
End of 57 train_loss: 0.4643, train_acc: 80.8882, val_loss: 0.8794, val_acc: 70.9659, f1_score: 0.6983
Epoch 58------------------------------------------------------------
19/190 train_loss: 0.478432, train_acc: 82.343750
39/190 train_loss: 0.491287, train_acc: 81.015625
59/190 train_loss: 0.488375, train_acc: 80.520833
79/190 train_loss: 0.496964, train_acc: 79.843750
99/190 train_loss: 0.488208, train_acc: 80.062500
119/190 train_loss: 0.489702, train_acc: 79.921875
139/190 train_loss: 0.482289, train_acc: 80.156250
159/190 train_loss: 0.478918, train_acc: 80.546875
179/190 train_loss: 0.479717, train_acc: 80.555556
End of 58 train_loss: 0.4754, train_acc: 80.7072, val_loss: 0.8480, val_acc: 70.6818, f1_score: 0.6942
Epoch 59------------------------------------------------------------
19/190 train_loss: 0.514652, train_acc: 80.156250
39/190 train_loss: 0.503538, train_acc: 81.562500
59/190 train_loss: 0.493943, train_acc: 81.197917
79/190 train_loss: 0.501210, train_acc: 80.039062
99/190 train_loss: 0.493639, train_acc: 79.750000
119/190 train_loss: 0.490070, train_acc: 80.026042
139/190 train_loss: 0.481440, train_acc: 80.513393
159/190 train_loss: 0.477079, train_acc: 80.742188
179/190 train_loss: 0.472709, train_acc: 80.902778
End of 59 train_loss: 0.4700, train_acc: 80.9704, val_loss: 0.8393, val_acc: 71.5341, f1_score: 0.7049
Epoch 60------------------------------------------------------------
19/190 train_loss: 0.492341, train_acc: 78.437500
39/190 train_loss: 0.487893, train_acc: 79.531250
59/190 train_loss: 0.482069, train_acc: 79.635417
79/190 train_loss: 0.483350, train_acc: 79.062500
99/190 train_loss: 0.480315, train_acc: 79.375000
119/190 train_loss: 0.485468, train_acc: 79.427083
139/190 train_loss: 0.479262, train_acc: 79.888393
159/190 train_loss: 0.479065, train_acc: 80.097656
179/190 train_loss: 0.473712, train_acc: 80.173611
End of 60 train_loss: 0.4708, train_acc: 80.2303, val_loss: 0.8636, val_acc: 71.9602, f1_score: 0.7079
Training ended with 60 epochs.
Loading best checkpoints from saved_models/Tweets/train/best_checkpoint.pt
Evaluation Results: test_loss:0.6911957263946533, test_acc:75.36931818181819, test_f1:0.7414515471754549
