Loading vocab...
Loading vocab from: ../dataset/Biaffine/glove/Restaurants/vocab_tok.vocab
Loading vocab from: ../dataset/Biaffine/glove/Restaurants/vocab_post.vocab
Loading vocab from: ../dataset/Biaffine/glove/Restaurants/vocab_pos.vocab
Loading vocab from: ../dataset/Biaffine/glove/Restaurants/vocab_dep.vocab
Loading vocab from: ../dataset/Biaffine/glove/Restaurants/vocab_pol.vocab
token_vocab: 4521, post_vocab: 160, pos_vocab: 46, dep_vocab: 35, pol_vocab: 3
Loading pretrained word emb...
Loading 4340/4521 words from vocab...
-----------  Configuration Arguments -----------
alpha: 1.0
att_dropout: 0
attn_heads: 10
batch_size: 32
beta: 1.0
bidirect: True
cross_val_fold: 10
data_dir: ../dataset/Biaffine/glove/Restaurants
dep_dim: 30
dep_size: 35
direct: False
emb_dim: 300
glove_dir: /mnt/data2/xfbai/data/embeddings/glove
hidden_dim: 50
input_dropout: 0.7
layer_dropout: 0
log: logs.txt
log_step: 20
loop: True
lower: True
lr: 0.01
model: RGAT
num_class: 3
num_epoch: 65
num_layers: 6
optim: adamax
output_merge: None
pooling: avg
pos_dim: 30
pos_size: 46
post_dim: 30
post_size: 160
rnn_dropout: 0.1
rnn_hidden: 50
rnn_layers: 1
save_dir: saved_models/Restaurants/train
seed: 14
shuffle: True
tok_size: 4521
tune: False
vocab_dir: ../dataset/Biaffine/glove/Restaurants
------------------------------------------------
3608 instances loaded from ../dataset/Biaffine/glove/Restaurants/train.json
113 batches created for ../dataset/Biaffine/glove/Restaurants/train.json
1119 instances loaded from ../dataset/Biaffine/glove/Restaurants/valid.json
35 batches created for ../dataset/Biaffine/glove/Restaurants/valid.json
1119 instances loaded from ../dataset/Biaffine/glove/Restaurants/test.json
35 batches created for ../dataset/Biaffine/glove/Restaurants/test.json
/mnt/data2/xfbai/Anaconda/envs/py36torch1.2new/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
RGATABSA(
  (enc): ABSAEncoder(
    (emb): Embedding(4521, 300, padding_idx=0)
    (pos_emb): Embedding(46, 30, padding_idx=0)
    (post_emb): Embedding(160, 30, padding_idx=0)
    (dep_emb): Embedding(35, 30, padding_idx=0)
    (encoder): DoubleEncoder(
      (emb): Embedding(4521, 300, padding_idx=0)
      (pos_emb): Embedding(46, 30, padding_idx=0)
      (post_emb): Embedding(160, 30, padding_idx=0)
      (dep_emb): Embedding(35, 30, padding_idx=0)
      (Sent_encoder): LSTM(360, 50, batch_first=True, dropout=0.1, bidirectional=True)
      (rnn_drop): Dropout(p=0.1, inplace=False)
      (in_drop): Dropout(p=0.7, inplace=False)
      (graph_encoder): RGATEncoder(
        (transformer): ModuleList(
          (0): RGATLayer(
            (self_attn): MultiHeadedAttention(
              (linear_keys): Linear(in_features=100, out_features=100, bias=True)
              (linear_values): Linear(in_features=100, out_features=100, bias=True)
              (linear_query): Linear(in_features=100, out_features=100, bias=True)
              (linear_structure_k): Linear(in_features=30, out_features=10, bias=True)
              (linear_structure_v): Linear(in_features=30, out_features=10, bias=True)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0, inplace=False)
              (final_linear): Linear(in_features=100, out_features=100, bias=True)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=100, out_features=100, bias=True)
              (w_2): Linear(in_features=100, out_features=100, bias=True)
              (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
              (dropout_1): Dropout(p=0, inplace=False)
              (relu): ReLU()
              (dropout_2): Dropout(p=0, inplace=False)
            )
            (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (1): RGATLayer(
            (self_attn): MultiHeadedAttention(
              (linear_keys): Linear(in_features=100, out_features=100, bias=True)
              (linear_values): Linear(in_features=100, out_features=100, bias=True)
              (linear_query): Linear(in_features=100, out_features=100, bias=True)
              (linear_structure_k): Linear(in_features=30, out_features=10, bias=True)
              (linear_structure_v): Linear(in_features=30, out_features=10, bias=True)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0, inplace=False)
              (final_linear): Linear(in_features=100, out_features=100, bias=True)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=100, out_features=100, bias=True)
              (w_2): Linear(in_features=100, out_features=100, bias=True)
              (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
              (dropout_1): Dropout(p=0, inplace=False)
              (relu): ReLU()
              (dropout_2): Dropout(p=0, inplace=False)
            )
            (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (2): RGATLayer(
            (self_attn): MultiHeadedAttention(
              (linear_keys): Linear(in_features=100, out_features=100, bias=True)
              (linear_values): Linear(in_features=100, out_features=100, bias=True)
              (linear_query): Linear(in_features=100, out_features=100, bias=True)
              (linear_structure_k): Linear(in_features=30, out_features=10, bias=True)
              (linear_structure_v): Linear(in_features=30, out_features=10, bias=True)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0, inplace=False)
              (final_linear): Linear(in_features=100, out_features=100, bias=True)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=100, out_features=100, bias=True)
              (w_2): Linear(in_features=100, out_features=100, bias=True)
              (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
              (dropout_1): Dropout(p=0, inplace=False)
              (relu): ReLU()
              (dropout_2): Dropout(p=0, inplace=False)
            )
            (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (3): RGATLayer(
            (self_attn): MultiHeadedAttention(
              (linear_keys): Linear(in_features=100, out_features=100, bias=True)
              (linear_values): Linear(in_features=100, out_features=100, bias=True)
              (linear_query): Linear(in_features=100, out_features=100, bias=True)
              (linear_structure_k): Linear(in_features=30, out_features=10, bias=True)
              (linear_structure_v): Linear(in_features=30, out_features=10, bias=True)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0, inplace=False)
              (final_linear): Linear(in_features=100, out_features=100, bias=True)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=100, out_features=100, bias=True)
              (w_2): Linear(in_features=100, out_features=100, bias=True)
              (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
              (dropout_1): Dropout(p=0, inplace=False)
              (relu): ReLU()
              (dropout_2): Dropout(p=0, inplace=False)
            )
            (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (4): RGATLayer(
            (self_attn): MultiHeadedAttention(
              (linear_keys): Linear(in_features=100, out_features=100, bias=True)
              (linear_values): Linear(in_features=100, out_features=100, bias=True)
              (linear_query): Linear(in_features=100, out_features=100, bias=True)
              (linear_structure_k): Linear(in_features=30, out_features=10, bias=True)
              (linear_structure_v): Linear(in_features=30, out_features=10, bias=True)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0, inplace=False)
              (final_linear): Linear(in_features=100, out_features=100, bias=True)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=100, out_features=100, bias=True)
              (w_2): Linear(in_features=100, out_features=100, bias=True)
              (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
              (dropout_1): Dropout(p=0, inplace=False)
              (relu): ReLU()
              (dropout_2): Dropout(p=0, inplace=False)
            )
            (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (5): RGATLayer(
            (self_attn): MultiHeadedAttention(
              (linear_keys): Linear(in_features=100, out_features=100, bias=True)
              (linear_values): Linear(in_features=100, out_features=100, bias=True)
              (linear_query): Linear(in_features=100, out_features=100, bias=True)
              (linear_structure_k): Linear(in_features=30, out_features=10, bias=True)
              (linear_structure_v): Linear(in_features=30, out_features=10, bias=True)
              (softmax): Softmax(dim=-1)
              (dropout): Dropout(p=0, inplace=False)
              (final_linear): Linear(in_features=100, out_features=100, bias=True)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=100, out_features=100, bias=True)
              (w_2): Linear(in_features=100, out_features=100, bias=True)
              (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
              (dropout_1): Dropout(p=0, inplace=False)
              (relu): ReLU()
              (dropout_2): Dropout(p=0, inplace=False)
            )
            (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
        (layer_norm): LayerNorm((100,), eps=1e-06, elementwise_affine=True)
      )
      (out_map): Linear(in_features=100, out_features=50, bias=True)
    )
  )
  (classifier): Linear(in_features=50, out_features=3, bias=True)
)
Total parameters: 1903453
Training Set: 113
Valid Set: 35
Test Set: 35
Epoch 1------------------------------------------------------------
19/113 train_loss: 1.047034, train_acc: 58.437500
39/113 train_loss: 1.000554, train_acc: 59.296875
59/113 train_loss: 1.001152, train_acc: 58.541667
79/113 train_loss: 0.985207, train_acc: 59.218750
99/113 train_loss: 0.980778, train_acc: 59.125000
End of 1 train_loss: 0.9769, train_acc: 59.1630, val_loss: 0.8781, val_acc: 64.9309, f1_score: 0.2625
new best model saved.
Epoch 2------------------------------------------------------------
19/113 train_loss: 0.899955, train_acc: 62.968750
39/113 train_loss: 0.900044, train_acc: 61.640625
59/113 train_loss: 0.908112, train_acc: 60.781250
79/113 train_loss: 0.884315, train_acc: 61.679688
99/113 train_loss: 0.862003, train_acc: 62.062500
End of 2 train_loss: 0.8521, train_acc: 62.5092, val_loss: 0.6522, val_acc: 73.8854, f1_score: 0.5804
new best model saved.
Epoch 3------------------------------------------------------------
19/113 train_loss: 0.748517, train_acc: 67.031250
39/113 train_loss: 0.755812, train_acc: 66.640625
59/113 train_loss: 0.769223, train_acc: 66.354167
79/113 train_loss: 0.760034, train_acc: 66.562500
99/113 train_loss: 0.743984, train_acc: 67.406250
End of 3 train_loss: 0.7396, train_acc: 67.8282, val_loss: 0.5752, val_acc: 76.8347, f1_score: 0.6481
new best model saved.
Epoch 4------------------------------------------------------------
19/113 train_loss: 0.679981, train_acc: 70.468750
39/113 train_loss: 0.692118, train_acc: 69.531250
59/113 train_loss: 0.709991, train_acc: 69.218750
79/113 train_loss: 0.704629, train_acc: 69.218750
99/113 train_loss: 0.693262, train_acc: 70.218750
End of 4 train_loss: 0.6885, train_acc: 70.4093, val_loss: 0.5899, val_acc: 76.9240, f1_score: 0.6609
new best model saved.
Epoch 5------------------------------------------------------------
19/113 train_loss: 0.663419, train_acc: 71.093750
39/113 train_loss: 0.684296, train_acc: 70.390625
59/113 train_loss: 0.688743, train_acc: 70.989583
79/113 train_loss: 0.679957, train_acc: 70.976562
99/113 train_loss: 0.669943, train_acc: 71.812500
End of 5 train_loss: 0.6685, train_acc: 71.6906, val_loss: 0.5564, val_acc: 77.4683, f1_score: 0.6779
new best model saved.
Epoch 6------------------------------------------------------------
19/113 train_loss: 0.627164, train_acc: 72.031250
39/113 train_loss: 0.644791, train_acc: 72.343750
59/113 train_loss: 0.654529, train_acc: 72.031250
79/113 train_loss: 0.647719, train_acc: 72.460938
99/113 train_loss: 0.644077, train_acc: 72.906250
End of 6 train_loss: 0.6409, train_acc: 72.9628, val_loss: 0.5395, val_acc: 78.1797, f1_score: 0.6767
new best model saved.
Epoch 7------------------------------------------------------------
19/113 train_loss: 0.594904, train_acc: 75.156250
39/113 train_loss: 0.635677, train_acc: 72.968750
59/113 train_loss: 0.647449, train_acc: 72.500000
79/113 train_loss: 0.637076, train_acc: 73.203125
99/113 train_loss: 0.630421, train_acc: 73.468750
End of 7 train_loss: 0.6268, train_acc: 73.7924, val_loss: 0.5317, val_acc: 80.0576, f1_score: 0.7097
new best model saved.
Epoch 8------------------------------------------------------------
19/113 train_loss: 0.608779, train_acc: 73.125000
39/113 train_loss: 0.628726, train_acc: 72.578125
59/113 train_loss: 0.631603, train_acc: 72.343750
79/113 train_loss: 0.618016, train_acc: 73.828125
99/113 train_loss: 0.613647, train_acc: 73.968750
End of 8 train_loss: 0.6117, train_acc: 73.9860, val_loss: 0.5052, val_acc: 81.0426, f1_score: 0.7314
new best model saved.
Epoch 9------------------------------------------------------------
19/113 train_loss: 0.592665, train_acc: 74.062500
39/113 train_loss: 0.613732, train_acc: 73.515625
59/113 train_loss: 0.616202, train_acc: 73.593750
79/113 train_loss: 0.607248, train_acc: 73.671875
99/113 train_loss: 0.607899, train_acc: 73.843750
End of 9 train_loss: 0.6030, train_acc: 74.1243, val_loss: 0.5198, val_acc: 80.8554, f1_score: 0.7225
Epoch 10------------------------------------------------------------
19/113 train_loss: 0.566829, train_acc: 75.625000
39/113 train_loss: 0.595011, train_acc: 74.687500
59/113 train_loss: 0.600055, train_acc: 74.531250
79/113 train_loss: 0.592515, train_acc: 75.000000
99/113 train_loss: 0.578901, train_acc: 75.593750
End of 10 train_loss: 0.5764, train_acc: 75.6545, val_loss: 0.5067, val_acc: 81.0397, f1_score: 0.7262
Epoch 11------------------------------------------------------------
19/113 train_loss: 0.556007, train_acc: 74.375000
39/113 train_loss: 0.579615, train_acc: 74.609375
59/113 train_loss: 0.579160, train_acc: 74.739583
79/113 train_loss: 0.570719, train_acc: 75.390625
99/113 train_loss: 0.571934, train_acc: 75.281250
End of 11 train_loss: 0.5692, train_acc: 75.7743, val_loss: 0.5259, val_acc: 80.7661, f1_score: 0.7143
Epoch 12------------------------------------------------------------
19/113 train_loss: 0.547138, train_acc: 77.968750
39/113 train_loss: 0.587705, train_acc: 75.937500
59/113 train_loss: 0.583768, train_acc: 76.093750
79/113 train_loss: 0.569596, train_acc: 76.367188
99/113 train_loss: 0.558309, train_acc: 76.718750
End of 12 train_loss: 0.5564, train_acc: 76.7699, val_loss: 0.4846, val_acc: 80.7776, f1_score: 0.7261
Epoch 13------------------------------------------------------------
19/113 train_loss: 0.532656, train_acc: 77.187500
39/113 train_loss: 0.572382, train_acc: 75.546875
59/113 train_loss: 0.574366, train_acc: 76.354167
79/113 train_loss: 0.557860, train_acc: 77.382812
99/113 train_loss: 0.556581, train_acc: 77.125000
End of 13 train_loss: 0.5570, train_acc: 77.0465, val_loss: 0.5280, val_acc: 80.7719, f1_score: 0.7150
Epoch 14------------------------------------------------------------
19/113 train_loss: 0.534966, train_acc: 78.281250
39/113 train_loss: 0.553687, train_acc: 76.796875
59/113 train_loss: 0.554017, train_acc: 76.875000
79/113 train_loss: 0.540582, train_acc: 77.773438
99/113 train_loss: 0.537781, train_acc: 77.812500
End of 14 train_loss: 0.5334, train_acc: 78.0420, val_loss: 0.5163, val_acc: 80.6855, f1_score: 0.7114
Epoch 15------------------------------------------------------------
19/113 train_loss: 0.535464, train_acc: 77.656250
39/113 train_loss: 0.559196, train_acc: 77.343750
59/113 train_loss: 0.550164, train_acc: 78.072917
79/113 train_loss: 0.534915, train_acc: 78.554688
99/113 train_loss: 0.526109, train_acc: 78.656250
End of 15 train_loss: 0.5212, train_acc: 79.0100, val_loss: 0.5189, val_acc: 81.1319, f1_score: 0.7320
new best model saved.
Epoch 16------------------------------------------------------------
19/113 train_loss: 0.507369, train_acc: 78.281250
39/113 train_loss: 0.529400, train_acc: 77.656250
59/113 train_loss: 0.530620, train_acc: 77.968750
79/113 train_loss: 0.522928, train_acc: 78.437500
99/113 train_loss: 0.515667, train_acc: 78.718750
End of 16 train_loss: 0.5146, train_acc: 78.9178, val_loss: 0.4812, val_acc: 82.1976, f1_score: 0.7380
new best model saved.
Epoch 17------------------------------------------------------------
19/113 train_loss: 0.459681, train_acc: 80.781250
39/113 train_loss: 0.506608, train_acc: 79.296875
59/113 train_loss: 0.511688, train_acc: 78.906250
79/113 train_loss: 0.503542, train_acc: 79.492188
99/113 train_loss: 0.499217, train_acc: 79.375000
End of 17 train_loss: 0.4985, train_acc: 79.4524, val_loss: 0.5355, val_acc: 80.9505, f1_score: 0.7160
Epoch 18------------------------------------------------------------
19/113 train_loss: 0.492990, train_acc: 79.375000
39/113 train_loss: 0.524258, train_acc: 78.046875
59/113 train_loss: 0.519310, train_acc: 78.906250
79/113 train_loss: 0.513564, train_acc: 79.023438
99/113 train_loss: 0.506149, train_acc: 79.156250
End of 18 train_loss: 0.5051, train_acc: 79.1482, val_loss: 0.4822, val_acc: 81.9297, f1_score: 0.7334
Epoch 19------------------------------------------------------------
19/113 train_loss: 0.486223, train_acc: 78.906250
39/113 train_loss: 0.506968, train_acc: 77.656250
59/113 train_loss: 0.497696, train_acc: 78.593750
79/113 train_loss: 0.492756, train_acc: 78.867188
99/113 train_loss: 0.485388, train_acc: 79.250000
End of 19 train_loss: 0.4903, train_acc: 79.2588, val_loss: 0.4979, val_acc: 80.0518, f1_score: 0.7005
Epoch 20------------------------------------------------------------
19/113 train_loss: 0.476651, train_acc: 79.218750
39/113 train_loss: 0.516527, train_acc: 78.515625
59/113 train_loss: 0.507356, train_acc: 78.906250
79/113 train_loss: 0.492924, train_acc: 79.609375
99/113 train_loss: 0.485997, train_acc: 79.843750
End of 20 train_loss: 0.4855, train_acc: 80.2083, val_loss: 0.5240, val_acc: 81.3940, f1_score: 0.7194
Epoch 21------------------------------------------------------------
19/113 train_loss: 0.434933, train_acc: 82.500000
39/113 train_loss: 0.474927, train_acc: 80.703125
59/113 train_loss: 0.488445, train_acc: 79.947917
79/113 train_loss: 0.480029, train_acc: 80.234375
99/113 train_loss: 0.473801, train_acc: 80.250000
End of 21 train_loss: 0.4683, train_acc: 80.4204, val_loss: 0.4877, val_acc: 82.4654, f1_score: 0.7473
new best model saved.
Epoch 22------------------------------------------------------------
19/113 train_loss: 0.448734, train_acc: 79.687500
39/113 train_loss: 0.480216, train_acc: 79.062500
59/113 train_loss: 0.477415, train_acc: 79.687500
79/113 train_loss: 0.473482, train_acc: 79.921875
99/113 train_loss: 0.473078, train_acc: 80.062500
End of 22 train_loss: 0.4709, train_acc: 80.0885, val_loss: 0.4887, val_acc: 82.3762, f1_score: 0.7503
Epoch 23------------------------------------------------------------
19/113 train_loss: 0.431191, train_acc: 80.156250
39/113 train_loss: 0.459558, train_acc: 80.156250
59/113 train_loss: 0.464651, train_acc: 79.739583
79/113 train_loss: 0.464217, train_acc: 79.804688
99/113 train_loss: 0.454854, train_acc: 80.343750
End of 23 train_loss: 0.4551, train_acc: 80.2268, val_loss: 0.4727, val_acc: 82.3819, f1_score: 0.7501
Epoch 24------------------------------------------------------------
19/113 train_loss: 0.478861, train_acc: 79.062500
39/113 train_loss: 0.485743, train_acc: 78.984375
59/113 train_loss: 0.478254, train_acc: 79.947917
79/113 train_loss: 0.467807, train_acc: 80.820312
99/113 train_loss: 0.469890, train_acc: 80.437500
End of 24 train_loss: 0.4685, train_acc: 80.2544, val_loss: 0.5057, val_acc: 83.5455, f1_score: 0.7599
new best model saved.
Epoch 25------------------------------------------------------------
19/113 train_loss: 0.405021, train_acc: 83.125000
39/113 train_loss: 0.463156, train_acc: 80.390625
59/113 train_loss: 0.460350, train_acc: 80.572917
79/113 train_loss: 0.452037, train_acc: 81.250000
99/113 train_loss: 0.446152, train_acc: 81.281250
End of 25 train_loss: 0.4396, train_acc: 81.7662, val_loss: 0.6113, val_acc: 81.3047, f1_score: 0.7164
Epoch 26------------------------------------------------------------
19/113 train_loss: 0.427081, train_acc: 82.343750
39/113 train_loss: 0.441745, train_acc: 81.796875
59/113 train_loss: 0.450212, train_acc: 82.031250
79/113 train_loss: 0.449646, train_acc: 82.109375
99/113 train_loss: 0.438367, train_acc: 82.156250
End of 26 train_loss: 0.4368, train_acc: 82.2917, val_loss: 0.5590, val_acc: 81.3911, f1_score: 0.7197
Epoch 27------------------------------------------------------------
19/113 train_loss: 0.426813, train_acc: 81.718750
39/113 train_loss: 0.454431, train_acc: 80.468750
59/113 train_loss: 0.449923, train_acc: 81.562500
79/113 train_loss: 0.439614, train_acc: 81.953125
99/113 train_loss: 0.436067, train_acc: 82.000000
End of 27 train_loss: 0.4393, train_acc: 81.8584, val_loss: 0.5207, val_acc: 81.4833, f1_score: 0.7272
Epoch 28------------------------------------------------------------
19/113 train_loss: 0.433564, train_acc: 83.593750
39/113 train_loss: 0.438566, train_acc: 82.890625
59/113 train_loss: 0.430925, train_acc: 83.072917
79/113 train_loss: 0.426670, train_acc: 83.242188
99/113 train_loss: 0.420607, train_acc: 83.250000
End of 28 train_loss: 0.4185, train_acc: 83.3518, val_loss: 0.5233, val_acc: 80.8612, f1_score: 0.7191
Epoch 29------------------------------------------------------------
19/113 train_loss: 0.417400, train_acc: 82.812500
39/113 train_loss: 0.433171, train_acc: 82.187500
59/113 train_loss: 0.434005, train_acc: 82.135417
79/113 train_loss: 0.428663, train_acc: 82.109375
99/113 train_loss: 0.422675, train_acc: 82.312500
End of 29 train_loss: 0.4223, train_acc: 82.3101, val_loss: 0.5239, val_acc: 81.7569, f1_score: 0.7383
Epoch 30------------------------------------------------------------
19/113 train_loss: 0.395698, train_acc: 84.687500
39/113 train_loss: 0.417508, train_acc: 83.593750
59/113 train_loss: 0.415008, train_acc: 83.906250
79/113 train_loss: 0.408782, train_acc: 83.945312
99/113 train_loss: 0.399474, train_acc: 84.406250
End of 30 train_loss: 0.4004, train_acc: 84.4211, val_loss: 0.6618, val_acc: 79.6976, f1_score: 0.6787
Epoch 31------------------------------------------------------------
19/113 train_loss: 0.416367, train_acc: 82.656250
39/113 train_loss: 0.435080, train_acc: 82.031250
59/113 train_loss: 0.421445, train_acc: 82.812500
79/113 train_loss: 0.428141, train_acc: 82.851562
99/113 train_loss: 0.412374, train_acc: 83.500000
End of 31 train_loss: 0.4089, train_acc: 83.4255, val_loss: 0.6549, val_acc: 79.5190, f1_score: 0.6894
Epoch 32------------------------------------------------------------
19/113 train_loss: 0.430284, train_acc: 82.500000
39/113 train_loss: 0.434690, train_acc: 82.343750
59/113 train_loss: 0.421064, train_acc: 83.177083
79/113 train_loss: 0.412562, train_acc: 83.593750
99/113 train_loss: 0.408147, train_acc: 83.937500
End of 32 train_loss: 0.4097, train_acc: 83.9049, val_loss: 0.5256, val_acc: 80.8554, f1_score: 0.7214
Epoch 33------------------------------------------------------------
19/113 train_loss: 0.395845, train_acc: 82.031250
39/113 train_loss: 0.422954, train_acc: 81.796875
59/113 train_loss: 0.404097, train_acc: 83.229167
79/113 train_loss: 0.405323, train_acc: 83.476562
99/113 train_loss: 0.402175, train_acc: 83.625000
End of 33 train_loss: 0.4000, train_acc: 83.6744, val_loss: 0.5552, val_acc: 80.7690, f1_score: 0.7206
Epoch 34------------------------------------------------------------
19/113 train_loss: 0.372596, train_acc: 84.218750
39/113 train_loss: 0.415240, train_acc: 82.812500
59/113 train_loss: 0.397575, train_acc: 83.489583
79/113 train_loss: 0.390097, train_acc: 84.101562
99/113 train_loss: 0.391060, train_acc: 84.031250
End of 34 train_loss: 0.3901, train_acc: 84.1169, val_loss: 0.5515, val_acc: 81.5755, f1_score: 0.7317
Epoch 35------------------------------------------------------------
19/113 train_loss: 0.386094, train_acc: 84.687500
39/113 train_loss: 0.390730, train_acc: 84.765625
59/113 train_loss: 0.385588, train_acc: 85.156250
79/113 train_loss: 0.372522, train_acc: 85.585938
99/113 train_loss: 0.370435, train_acc: 85.437500
End of 35 train_loss: 0.3732, train_acc: 85.3060, val_loss: 0.5649, val_acc: 80.5933, f1_score: 0.7138
Epoch 36------------------------------------------------------------
19/113 train_loss: 0.382374, train_acc: 82.968750
39/113 train_loss: 0.397865, train_acc: 82.656250
59/113 train_loss: 0.377791, train_acc: 83.854167
79/113 train_loss: 0.362744, train_acc: 84.570312
99/113 train_loss: 0.359730, train_acc: 84.812500
End of 36 train_loss: 0.3569, train_acc: 84.8544, val_loss: 0.6348, val_acc: 80.6826, f1_score: 0.7137
Epoch 37------------------------------------------------------------
19/113 train_loss: 0.379498, train_acc: 84.687500
39/113 train_loss: 0.396871, train_acc: 84.218750
59/113 train_loss: 0.387466, train_acc: 84.270833
79/113 train_loss: 0.386655, train_acc: 84.414062
99/113 train_loss: 0.379361, train_acc: 84.468750
End of 37 train_loss: 0.3813, train_acc: 84.3013, val_loss: 0.5221, val_acc: 81.1319, f1_score: 0.7236
Epoch 38------------------------------------------------------------
19/113 train_loss: 0.392951, train_acc: 82.500000
39/113 train_loss: 0.399883, train_acc: 82.734375
59/113 train_loss: 0.396085, train_acc: 83.229167
79/113 train_loss: 0.396113, train_acc: 83.437500
99/113 train_loss: 0.379488, train_acc: 84.312500
End of 38 train_loss: 0.3780, train_acc: 84.4395, val_loss: 0.6128, val_acc: 81.3076, f1_score: 0.7259
Epoch 39------------------------------------------------------------
19/113 train_loss: 0.354637, train_acc: 85.312500
39/113 train_loss: 0.383605, train_acc: 84.531250
59/113 train_loss: 0.389038, train_acc: 84.218750
79/113 train_loss: 0.372004, train_acc: 85.078125
99/113 train_loss: 0.360212, train_acc: 85.406250
End of 39 train_loss: 0.3615, train_acc: 85.0572, val_loss: 0.6080, val_acc: 81.1290, f1_score: 0.7058
Epoch 40------------------------------------------------------------
19/113 train_loss: 0.332333, train_acc: 86.875000
39/113 train_loss: 0.367837, train_acc: 85.390625
59/113 train_loss: 0.355906, train_acc: 85.781250
79/113 train_loss: 0.357371, train_acc: 85.781250
99/113 train_loss: 0.351736, train_acc: 86.312500
End of 40 train_loss: 0.3544, train_acc: 86.0066, val_loss: 0.5617, val_acc: 80.8583, f1_score: 0.7096
Epoch 41------------------------------------------------------------
19/113 train_loss: 0.351292, train_acc: 85.625000
39/113 train_loss: 0.375396, train_acc: 84.296875
59/113 train_loss: 0.370002, train_acc: 84.531250
79/113 train_loss: 0.363873, train_acc: 85.039062
99/113 train_loss: 0.351642, train_acc: 85.406250
End of 41 train_loss: 0.3533, train_acc: 85.4443, val_loss: 0.6366, val_acc: 80.2362, f1_score: 0.7009
Epoch 42------------------------------------------------------------
19/113 train_loss: 0.357482, train_acc: 84.843750
39/113 train_loss: 0.365540, train_acc: 84.843750
59/113 train_loss: 0.358206, train_acc: 85.520833
79/113 train_loss: 0.356392, train_acc: 85.585938
99/113 train_loss: 0.358111, train_acc: 85.500000
End of 42 train_loss: 0.3540, train_acc: 85.7024, val_loss: 0.6447, val_acc: 79.9654, f1_score: 0.6952
Epoch 43------------------------------------------------------------
19/113 train_loss: 0.344481, train_acc: 85.781250
39/113 train_loss: 0.342588, train_acc: 85.703125
59/113 train_loss: 0.346256, train_acc: 85.677083
79/113 train_loss: 0.341190, train_acc: 85.859375
99/113 train_loss: 0.340267, train_acc: 85.843750
End of 43 train_loss: 0.3429, train_acc: 85.7301, val_loss: 0.6399, val_acc: 79.8762, f1_score: 0.6918
Epoch 44------------------------------------------------------------
19/113 train_loss: 0.373438, train_acc: 82.968750
39/113 train_loss: 0.373850, train_acc: 83.906250
59/113 train_loss: 0.355978, train_acc: 85.416667
79/113 train_loss: 0.346871, train_acc: 85.742188
99/113 train_loss: 0.344420, train_acc: 85.593750
End of 44 train_loss: 0.3382, train_acc: 85.8684, val_loss: 0.6854, val_acc: 80.5962, f1_score: 0.7137
Epoch 45------------------------------------------------------------
19/113 train_loss: 0.360778, train_acc: 85.468750
39/113 train_loss: 0.368728, train_acc: 84.531250
59/113 train_loss: 0.359501, train_acc: 85.156250
79/113 train_loss: 0.349974, train_acc: 85.742188
99/113 train_loss: 0.344951, train_acc: 85.812500
End of 45 train_loss: 0.3407, train_acc: 86.0159, val_loss: 0.7048, val_acc: 79.7033, f1_score: 0.6871
Epoch 46------------------------------------------------------------
19/113 train_loss: 0.338211, train_acc: 87.031250
39/113 train_loss: 0.354174, train_acc: 86.328125
59/113 train_loss: 0.347005, train_acc: 85.833333
79/113 train_loss: 0.348502, train_acc: 85.937500
99/113 train_loss: 0.344069, train_acc: 86.031250
End of 46 train_loss: 0.3409, train_acc: 86.1080, val_loss: 0.6978, val_acc: 80.5962, f1_score: 0.7054
Epoch 47------------------------------------------------------------
19/113 train_loss: 0.331538, train_acc: 86.718750
39/113 train_loss: 0.347926, train_acc: 86.171875
59/113 train_loss: 0.336138, train_acc: 86.822917
79/113 train_loss: 0.340267, train_acc: 86.640625
99/113 train_loss: 0.329814, train_acc: 86.781250
End of 47 train_loss: 0.3324, train_acc: 86.8455, val_loss: 0.6072, val_acc: 79.8819, f1_score: 0.7004
Epoch 48------------------------------------------------------------
19/113 train_loss: 0.339045, train_acc: 84.843750
39/113 train_loss: 0.342456, train_acc: 85.625000
59/113 train_loss: 0.334143, train_acc: 85.937500
79/113 train_loss: 0.331545, train_acc: 85.976562
99/113 train_loss: 0.326988, train_acc: 86.156250
End of 48 train_loss: 0.3280, train_acc: 85.9052, val_loss: 0.6517, val_acc: 79.8819, f1_score: 0.7026
Epoch 49------------------------------------------------------------
19/113 train_loss: 0.327565, train_acc: 86.875000
39/113 train_loss: 0.344764, train_acc: 86.093750
59/113 train_loss: 0.319824, train_acc: 87.395833
79/113 train_loss: 0.326542, train_acc: 87.187500
99/113 train_loss: 0.323297, train_acc: 87.375000
End of 49 train_loss: 0.3191, train_acc: 87.3249, val_loss: 0.7434, val_acc: 81.3134, f1_score: 0.7243
Epoch 50------------------------------------------------------------
19/113 train_loss: 0.354356, train_acc: 85.312500
39/113 train_loss: 0.362603, train_acc: 84.921875
59/113 train_loss: 0.349764, train_acc: 85.572917
79/113 train_loss: 0.332237, train_acc: 86.367188
99/113 train_loss: 0.324046, train_acc: 86.750000
End of 50 train_loss: 0.3195, train_acc: 86.8916, val_loss: 0.6979, val_acc: 80.4147, f1_score: 0.7081
Epoch 51------------------------------------------------------------
19/113 train_loss: 0.331948, train_acc: 87.187500
39/113 train_loss: 0.333992, train_acc: 87.265625
59/113 train_loss: 0.331264, train_acc: 86.822917
79/113 train_loss: 0.325008, train_acc: 87.500000
99/113 train_loss: 0.314566, train_acc: 87.625000
End of 51 train_loss: 0.3107, train_acc: 87.6291, val_loss: 0.7927, val_acc: 80.2419, f1_score: 0.6972
Epoch 52------------------------------------------------------------
19/113 train_loss: 0.299875, train_acc: 87.968750
39/113 train_loss: 0.329883, train_acc: 86.718750
59/113 train_loss: 0.320063, train_acc: 87.656250
79/113 train_loss: 0.310494, train_acc: 88.164062
99/113 train_loss: 0.313616, train_acc: 88.062500
End of 52 train_loss: 0.3080, train_acc: 88.0070, val_loss: 0.6730, val_acc: 80.1526, f1_score: 0.7108
Epoch 53------------------------------------------------------------
19/113 train_loss: 0.289893, train_acc: 87.656250
39/113 train_loss: 0.301634, train_acc: 87.734375
59/113 train_loss: 0.304527, train_acc: 87.187500
79/113 train_loss: 0.300258, train_acc: 87.148438
99/113 train_loss: 0.297435, train_acc: 87.468750
End of 53 train_loss: 0.2977, train_acc: 87.6198, val_loss: 0.6779, val_acc: 80.1498, f1_score: 0.7070
Epoch 54------------------------------------------------------------
19/113 train_loss: 0.316087, train_acc: 88.125000
39/113 train_loss: 0.345308, train_acc: 86.718750
59/113 train_loss: 0.324761, train_acc: 87.343750
79/113 train_loss: 0.309753, train_acc: 87.851562
99/113 train_loss: 0.313109, train_acc: 87.781250
End of 54 train_loss: 0.3092, train_acc: 87.8042, val_loss: 0.6038, val_acc: 79.3491, f1_score: 0.7007
Epoch 55------------------------------------------------------------
19/113 train_loss: 0.298846, train_acc: 88.281250
39/113 train_loss: 0.303783, train_acc: 87.656250
59/113 train_loss: 0.308868, train_acc: 87.864583
79/113 train_loss: 0.296042, train_acc: 88.007812
99/113 train_loss: 0.288336, train_acc: 88.156250
End of 55 train_loss: 0.2888, train_acc: 88.0715, val_loss: 0.6844, val_acc: 79.7062, f1_score: 0.7036
Epoch 56------------------------------------------------------------
19/113 train_loss: 0.273994, train_acc: 87.343750
39/113 train_loss: 0.296802, train_acc: 87.812500
59/113 train_loss: 0.304732, train_acc: 87.968750
79/113 train_loss: 0.301777, train_acc: 88.085938
99/113 train_loss: 0.297634, train_acc: 88.406250
End of 56 train_loss: 0.2957, train_acc: 88.4864, val_loss: 0.6889, val_acc: 79.4384, f1_score: 0.6953
Epoch 57------------------------------------------------------------
19/113 train_loss: 0.294598, train_acc: 89.218750
39/113 train_loss: 0.298381, train_acc: 88.281250
59/113 train_loss: 0.298588, train_acc: 88.489583
79/113 train_loss: 0.286578, train_acc: 88.750000
99/113 train_loss: 0.280502, train_acc: 89.062500
End of 57 train_loss: 0.2809, train_acc: 88.9473, val_loss: 0.6438, val_acc: 79.8790, f1_score: 0.7028
Epoch 58------------------------------------------------------------
19/113 train_loss: 0.323217, train_acc: 87.500000
39/113 train_loss: 0.319240, train_acc: 87.343750
59/113 train_loss: 0.304234, train_acc: 88.333333
79/113 train_loss: 0.297077, train_acc: 88.828125
99/113 train_loss: 0.295230, train_acc: 88.750000
End of 58 train_loss: 0.2885, train_acc: 88.8735, val_loss: 0.7073, val_acc: 80.3255, f1_score: 0.7137
Epoch 59------------------------------------------------------------
19/113 train_loss: 0.271121, train_acc: 88.281250
39/113 train_loss: 0.301538, train_acc: 87.187500
59/113 train_loss: 0.297123, train_acc: 87.916667
79/113 train_loss: 0.291021, train_acc: 88.203125
99/113 train_loss: 0.278781, train_acc: 88.593750
End of 59 train_loss: 0.2740, train_acc: 88.7168, val_loss: 0.8576, val_acc: 79.3433, f1_score: 0.6963
Epoch 60------------------------------------------------------------
19/113 train_loss: 0.271061, train_acc: 89.375000
39/113 train_loss: 0.286800, train_acc: 88.359375
59/113 train_loss: 0.284349, train_acc: 88.750000
79/113 train_loss: 0.281217, train_acc: 88.828125
99/113 train_loss: 0.273815, train_acc: 89.125000
End of 60 train_loss: 0.2730, train_acc: 88.9841, val_loss: 0.7473, val_acc: 80.4147, f1_score: 0.7063
Epoch 61------------------------------------------------------------
19/113 train_loss: 0.251915, train_acc: 90.156250
39/113 train_loss: 0.260427, train_acc: 89.765625
59/113 train_loss: 0.255972, train_acc: 89.895833
79/113 train_loss: 0.255829, train_acc: 89.804688
99/113 train_loss: 0.257969, train_acc: 89.968750
End of 61 train_loss: 0.2651, train_acc: 89.5926, val_loss: 0.6586, val_acc: 79.3376, f1_score: 0.6996
Epoch 62------------------------------------------------------------
19/113 train_loss: 0.265642, train_acc: 88.906250
39/113 train_loss: 0.297605, train_acc: 88.281250
59/113 train_loss: 0.279925, train_acc: 89.218750
79/113 train_loss: 0.279467, train_acc: 89.101562
99/113 train_loss: 0.271113, train_acc: 89.562500
End of 62 train_loss: 0.2708, train_acc: 89.4451, val_loss: 0.7002, val_acc: 79.7005, f1_score: 0.7002
Epoch 63------------------------------------------------------------
19/113 train_loss: 0.292027, train_acc: 88.281250
39/113 train_loss: 0.267168, train_acc: 89.062500
59/113 train_loss: 0.259634, train_acc: 89.791667
79/113 train_loss: 0.267101, train_acc: 89.492188
99/113 train_loss: 0.266103, train_acc: 89.531250
End of 63 train_loss: 0.2680, train_acc: 89.5372, val_loss: 0.6718, val_acc: 80.7690, f1_score: 0.7195
Epoch 64------------------------------------------------------------
19/113 train_loss: 0.247243, train_acc: 91.093750
39/113 train_loss: 0.259579, train_acc: 90.468750
59/113 train_loss: 0.265673, train_acc: 90.208333
79/113 train_loss: 0.267648, train_acc: 89.648438
99/113 train_loss: 0.266599, train_acc: 89.500000
End of 64 train_loss: 0.2653, train_acc: 89.4819, val_loss: 0.7280, val_acc: 80.0605, f1_score: 0.7159
Epoch 65------------------------------------------------------------
19/113 train_loss: 0.244928, train_acc: 90.468750
39/113 train_loss: 0.290867, train_acc: 88.203125
59/113 train_loss: 0.281373, train_acc: 88.906250
79/113 train_loss: 0.275093, train_acc: 89.140625
99/113 train_loss: 0.273481, train_acc: 89.125000
End of 65 train_loss: 0.2689, train_acc: 89.2607, val_loss: 0.7395, val_acc: 80.8641, f1_score: 0.7230
Training ended with 65 epochs.
Loading best checkpoints from saved_models/Restaurants/train/best_checkpoint.pt
Evaluation Results: test_loss:0.5056822299957275, test_acc:83.54550691244239, test_f1:0.7599042338001986
